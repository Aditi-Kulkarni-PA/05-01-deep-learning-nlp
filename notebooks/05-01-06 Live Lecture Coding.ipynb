{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc8192c",
   "metadata": {},
   "source": [
    "**Use Cases**\n",
    "Text preprocessing, Text cleaning, Spell Correction, String Similarity (Semantic), Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460c12d",
   "metadata": {},
   "source": [
    "| **Stage**                | **Included Tasks**                                                    | **Output Example**                            |\n",
    "| ------------------------ | --------------------------------------------------------------------- | --------------------------------------------- |\n",
    "| **Lexical Processing**   | Normalization, Tokenization, Stopword Removal, Morphological Analysis | ‚ÄúThe cats are running.‚Äù ‚Üí [the, cat, are, run] |\n",
    "| **Syntactic Processing** | POS Tagging, Chunking, Parsing, Agreement Checking                    | [NP The cats] [VP are running]                |\n",
    "| **Semantic Processing**  | Role Labelling, Disambiguation, Coreference, Inference                | agent(cats), action(run), time(now)           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0e091",
   "metadata": {},
   "source": [
    "- Lexical - \"Lexicon\" - words - Order, grammer, context does not matter\n",
    "- Syntactic - \"Syntax\" - Order, grammer matters (POS tags, NER tags)\n",
    "- Semantic - \"Semantics\" - Similarity - Order, grammer, context everything matters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77ef97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate ai-lab\n",
    "# conda install -c conda-forge nltk jellyfish scipy python-levenshtein\n",
    "# python -m spacy download en_core_web_md\n",
    "\n",
    "# !pip install nltk jellyfish scipy python-Levenshtein\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9fdb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk  # natural language toolkit\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe7a8cc",
   "metadata": {},
   "source": [
    "**Different Libraries**\n",
    "- spaCy and nltk are both for same purpose. Many things are common.\n",
    "- nltk has some more functionality.\n",
    "- Different organizations have developed these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6cc5b",
   "metadata": {},
   "source": [
    "Actions we will perform: tokenization, stemming, lemitization, punctuation removal, case folding, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe712a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\"Just finished reading an amazing book on AI ü§ñüìö #MachineLearning #AI.\",\n",
    "          \"What a beautiful morning! ‚òÄÔ∏è Feeling super motivated to start the day üí™ #GoodVibes\",\n",
    "          \"Ugh...@user my laptop crashed again right before my deadline üò§ #MondayBlues\",\n",
    "          \"brb grabbing some ‚òïÔ∏è lol can‚Äôt start work without caffeine üòÇ\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c6b915",
   "metadata": {},
   "source": [
    "**Models from nltk**\n",
    "- Download required models from nltk. These are not libraries.\n",
    "- In nltk, there are hundreds of models. These are heavy files and will consume memory.\n",
    "- So we have the provision to download only those that are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345c0082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('wordnet') # for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503cf271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweets:\n",
      "Just finished reading an amazing book on AI ü§ñüìö #MachineLearning #AI.\n",
      "What a beautiful morning! ‚òÄÔ∏è Feeling super motivated to start the day üí™ #GoodVibes\n",
      "Ugh...@user my laptop crashed again right before my deadline üò§ #MondayBlues\n",
      "brb grabbing some ‚òïÔ∏è lol can‚Äôt start work without caffeine üòÇ\n"
     ]
    }
   ],
   "source": [
    "print (\"Original Tweets:\")\n",
    "for t in tweets:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf69a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. CASE FOLDING - Convert text to lower case\n",
    "\n",
    "def case_folding(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3ca6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Punctuation in String Library:\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# B. REMOVE PUNCTUATIONS: ! # @ $ % ^ & * ( ) _ + - = { } [ ] | \\ : ; \" ' < > , . ? /\n",
    "# Remove only the needed punctauations, not all\n",
    "\n",
    "print (\"Default Punctuation in String Library:\")\n",
    "print (string.punctuation)\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7486c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. TOKENIZATION - Split text into words or tokens\n",
    "# Breaking the text into smaller units - words, phrases, sentences from paragraphs, or other meaningful elements\n",
    "\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0060bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. STEMMING - Reduce words to their root form\n",
    "# Remove prefixes, suffixes to get to the base or root form of a word\n",
    "# Rule based, fast, but may not produce actual words\n",
    "# Porter Stemmer and Snowball Stemmer are popular algorithms\n",
    "# Porter Stemmer is more aggressive, Snowball Stemmer is more accurate but slower\n",
    "\n",
    "def porter_stemming(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = tokenize(text)\n",
    "    return ' '.join(ps.stem(word) for word in words)\n",
    "\n",
    "def snowball_stemming(text):\n",
    "    ss = SnowballStemmer(\"english\")\n",
    "    words = tokenize(text)\n",
    "    return ' '.join(ss.stem(word) for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3207b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E. LEMMATIZATION - Reduce words to their base or dictionary form\n",
    "# Uses vocabulary and morphological analysis of words\n",
    "# More accurate than stemming, but slower\n",
    "# WordNet is a popular lexical database for English\n",
    "# Unless you are passing a wrongly spelled word, lemmatization will always return a valid word\n",
    "\n",
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = tokenize(text)\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in words)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Tweet: Just finished reading an amazing book on AI ü§ñüìö #MachineLearning #AI.\n",
      "After Case Folding: just finished reading an amazing book on ai ü§ñüìö #machinelearning #ai.\n",
      "After Removing Punctuations: just finished reading an amazing book on ai ü§ñüìö machinelearning ai\n",
      "After Porter Stemming: just finish read an amaz book on ai ü§ñüìö machinelearn ai\n",
      "After Snowball Stemming: just finish read an amaz book on ai ü§ñüìö machinelearn ai\n",
      "After Lemmatization: just finished reading an amazing book on ai ü§ñüìö machinelearning ai\n",
      "\n",
      "Original Tweet: What a beautiful morning! ‚òÄÔ∏è Feeling super motivated to start the day üí™ #GoodVibes\n",
      "After Case Folding: what a beautiful morning! ‚òÄÔ∏è feeling super motivated to start the day üí™ #goodvibes\n",
      "After Removing Punctuations: what a beautiful morning ‚òÄÔ∏è feeling super motivated to start the day üí™ goodvibes\n",
      "After Porter Stemming: what a beauti morn ‚òÄÔ∏è feel super motiv to start the day üí™ goodvib\n",
      "After Snowball Stemming: what a beauti morn ‚òÄÔ∏è feel super motiv to start the day üí™ goodvib\n",
      "After Lemmatization: what a beautiful morning ‚òÄÔ∏è feeling super motivated to start the day üí™ goodvibes\n",
      "\n",
      "Original Tweet: Ugh...@user my laptop crashed again right before my deadline üò§ #MondayBlues\n",
      "After Case Folding: ugh...@user my laptop crashed again right before my deadline üò§ #mondayblues\n",
      "After Removing Punctuations: ughuser my laptop crashed again right before my deadline üò§ mondayblues\n",
      "After Porter Stemming: ughus my laptop crash again right befor my deadlin üò§ mondayblu\n",
      "After Snowball Stemming: ughus my laptop crash again right befor my deadlin üò§ mondayblu\n",
      "After Lemmatization: ughuser my laptop crashed again right before my deadline üò§ mondayblues\n",
      "\n",
      "Original Tweet: brb grabbing some ‚òïÔ∏è lol can‚Äôt start work without caffeine üòÇ\n",
      "After Case Folding: brb grabbing some ‚òïÔ∏è lol can‚Äôt start work without caffeine üòÇ\n",
      "After Removing Punctuations: brb grabbing some ‚òïÔ∏è lol can‚Äôt start work without caffeine üòÇ\n",
      "After Porter Stemming: brb grab some ‚òïÔ∏è lol can ‚Äô t start work without caffein üòÇ\n",
      "After Snowball Stemming: brb grab some ‚òïÔ∏è lol can ‚Äô t start work without caffein üòÇ\n",
      "After Lemmatization: brb grabbing some ‚òïÔ∏è lol can ‚Äô t start work without caffeine üòÇ\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    print(\"\\nOriginal Tweet:\", tweet)\n",
    "\n",
    "    # A. Case Folding\n",
    "    tweet = case_folding(tweet)\n",
    "    print(\"After Case Folding:\", tweet)\n",
    "\n",
    "    # B. Remove Punctuations\n",
    "    tweet_out = remove_punctuations(tweet)\n",
    "    print(\"After Removing Punctuations:\", tweet_out)\n",
    "\n",
    "    \n",
    "\n",
    "    # C. Tokenization\n",
    "    #tokens = tokenize(tweet)\n",
    "    #print(\"After Tokenization:\")\n",
    "    #print(tokens)\n",
    "\n",
    "    # D. Stemming\n",
    "    porter_stemmed = porter_stemming(tweet_out)\n",
    "    snowball_stemmed = snowball_stemming(tweet_out)\n",
    "    print(\"After Porter Stemming:\", porter_stemmed)\n",
    "    print(\"After Snowball Stemming:\", snowball_stemmed)\n",
    "\n",
    "    # E. Lemmatization\n",
    "    lemmatized = lemmatization(tweet_out)\n",
    "    print(\"After Lemmatization:\", lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301ebe8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f428bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edit Distance between 'kitten' and 'sitting': 3\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "\n",
    "s1 = \"kitten\"\n",
    "s2 = \"sitting\"\n",
    "\n",
    "distance = edit_distance(s1, s2)\n",
    "print(f\"\\nEdit Distance between '{s1}' and '{s2}': {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cd27439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance between 'kitten' and 'sitting': 3\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "lev_distance = Levenshtein.distance(s1, s2)\n",
    "print(f\"Levenshtein Distance between '{s1}' and '{s2}': {lev_distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b838d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy python-Levenshtein\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25889775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance: 0.69406223\n",
      "Hamming distance (on binarized vectors): 0.4633333333333333\n",
      "spaCy similarity (cosine-like): 0.305937796831131\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scipy.spatial.distance import hamming, cosine\n",
    "import numpy as np\n",
    "\n",
    "# Load a model with vectors (en_core_web_md or en_core_web_lg)\n",
    "nlp = spacy.load(\"en_core_web_md\")   # download if you don't have it\n",
    "\n",
    "doc1 = nlp(\"kitten\")\n",
    "doc2 = nlp(\"sitting\")\n",
    "\n",
    "v1 = doc1.vector\n",
    "v2 = doc2.vector\n",
    "\n",
    "# Cosine distance (recommended for dense embeddings)\n",
    "cos_dist = cosine(v1, v2)\n",
    "print(\"Cosine distance:\", cos_dist)\n",
    "\n",
    "# Hamming distance = proportion of differing elements.\n",
    "# Hamming is designed for binary/boolean vectors ‚Äî convert floats to binary using a threshold if needed.\n",
    "b1 = (v1 > 0).astype(int)\n",
    "b2 = (v2 > 0).astype(int)\n",
    "hamming_dist = hamming(b1, b2)\n",
    "print(\"Hamming distance (on binarized vectors):\", hamming_dist)\n",
    "\n",
    "# Alternatively, spaCy provides a convenient .similarity() which uses cosine:\n",
    "print(\"spaCy similarity (cosine-like):\", doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1e2b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity (BOW): 0.6123724356957946\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def bow_vector(text, vocab):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    c = Counter(tokens)\n",
    "    return np.array([c[w] for w in vocab], dtype=float)\n",
    "\n",
    "a = \"I love machine learning\"\n",
    "b = \"I enjoy machine learning and AI\"\n",
    "vocab = sorted(set(nltk.word_tokenize(a.lower()) + nltk.word_tokenize(b.lower())))\n",
    "va = bow_vector(a, vocab)\n",
    "vb = bow_vector(b, vocab)\n",
    "\n",
    "sim = np.dot(va, vb) / (np.linalg.norm(va) * np.linalg.norm(vb)) if np.linalg.norm(va) and np.linalg.norm(vb) else 0.0\n",
    "print(\"cosine similarity (BOW):\", sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702fb9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Compare lemmatizers with pos=v (verbs) and pos=a (adjective) for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0125b889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ['running', 'runs', 'ran', 'better', 'cars']\n",
      "Porter:    ['run', 'run', 'ran', 'better', 'car']\n",
      "Lancaster: ['run', 'run', 'ran', 'bet', 'car']\n",
      "Snowball:  ['run', 'run', 'ran', 'better', 'car']\n",
      "Regexp:    ['runn', 'run', 'ran', 'better', 'car']\n",
      "WordNet (default):  ['running', 'run', 'ran', 'better', 'car']\n",
      "WordNet (as verbs): ['run', 'run', 'run', 'better', 'cars']\n",
      "WordNet (adj).    : good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# If you haven't already:\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # optional, improves WordNet coverage\n",
    "\n",
    "words = [\"running\", \"runs\", \"ran\", \"better\", \"cars\"]\n",
    "print(\"Original: \", words)\n",
    "\n",
    "# Stemmers\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "ss = SnowballStemmer(\"english\")\n",
    "rs = RegexpStemmer('ing$|s$|e$')  # simple example regexp\n",
    "\n",
    "print(\"Porter:   \", [ps.stem(w) for w in words])\n",
    "print(\"Lancaster:\", [ls.stem(w) for w in words])\n",
    "print(\"Snowball: \", [ss.stem(w) for w in words])\n",
    "print(\"Regexp:   \", [rs.stem(w) for w in words])\n",
    "\n",
    "# Lemmatizer (WordNet)\n",
    "wn = WordNetLemmatizer()\n",
    "print(\"WordNet (default): \", [wn.lemmatize(w) for w in words])\n",
    "# Pass POS for better results (verbs)\n",
    "print(\"WordNet (as verbs):\", [wn.lemmatize(w, pos='v') for w in words])\n",
    "# Comparative adjective handling (better -> good)\n",
    "print(\"WordNet (adj).    :\", wn.lemmatize(\"better\", pos='a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439521dc",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e55472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
