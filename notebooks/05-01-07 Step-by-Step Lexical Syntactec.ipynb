{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76618bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet_pipeline.py\n",
    "# Run with: python tweet_pipeline.py\n",
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "import string \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e634702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditikulkarni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ensure minimal NLTK data (will download if missing)\n",
    "for pkg in (\"punkt\", \"averaged_perceptron_tagger\", \"wordnet\", \"omw-1.4\", \"stopwords\"):\n",
    "    try:\n",
    "        nltk.data.find(pkg)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg.split(\"/\")[-1] if \"/\" in pkg else pkg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd910d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "tweets = [\n",
    "    \"Just finished reading an amazing book on AI ğŸ¤–ğŸ“š #MachineLearning #AI.\",\n",
    "    \"What a beautiful morning! â˜€ï¸ Feeling super motivated to start the day ğŸ’ª #GoodVibes\",\n",
    "    \"Ugh...@user my laptop crashed again right before my deadline ğŸ˜¤ #MondayBlues\",\n",
    "    \"brb grabbing some â˜•ï¸ lol canâ€™t start work without caffeine ğŸ˜‚\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6946c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small contraction map sufficient for demo\n",
    "contraction_map = {\n",
    "    \"can't\": \"can not\", \"cant\": \"can not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"didn't\": \"did not\", \"didnt\": \"did not\",\n",
    "    \"don't\": \"do not\", \"dont\": \"do not\",\n",
    "    \"can't\": \"can not\", \"can't\": \"can not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"canâ€™t\": \"can not\", \"didnâ€™t\": \"did not\", \"donâ€™t\": \"do not\",\n",
    "    \"brb\": \"be right back\", \"lol\": \"laughing out loud\", \"u\": \"you\"\n",
    "}\n",
    "\n",
    "# emoji map (small)\n",
    "emoji_map = {\n",
    "    \"ğŸ¤–\": \"robot\", \"ğŸ“š\": \"books\", \"â˜€ï¸\": \"sun\", \"ğŸ’ª\": \"strong\",\n",
    "    \"ğŸ˜¤\": \"annoyed\", \"â˜•ï¸\": \"coffee\", \"ğŸ˜‚\": \"laugh\", \"ğŸ˜Š\": \"smile\"\n",
    "}\n",
    "\n",
    "# chunk grammar (simple)\n",
    "GRAMMAR = r\"\"\"\n",
    "  NP: {<DT|PRP\\$>?<JJ.*>*<NN.*>+}   # determiner/adjectives + nouns\n",
    "  PP: {<IN><NP>}                    # preposition + NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$}      # verb phrase (simple)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# stemmers / lemmatizer\n",
    "porter = PorterStemmer()\n",
    "snow = SnowballStemmer(\"english\")\n",
    "lanc = LancasterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# chunker\n",
    "chunker = RegexpParser(GRAMMAR)\n",
    "chunk = \n",
    "\n",
    "# helper functions\n",
    "def replace_emojis(text):\n",
    "    for emoji, word in emoji_map.items():\n",
    "        text = text.replace(emoji, \" \" + word + \" \")\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for short, long in contraction_map.items():\n",
    "        text = re.sub(r\"\\b\" + re.escape(short) + r\"\\b\", long, text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # keep hashtags and mentions\n",
    "    for c in text:\n",
    "        if c in string.punctuation and c not in (\"#\",\"@\"):\n",
    "            text = text.replace(c, \" \")\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    # lower case, emoji -> word, expand contractions, separate punctuation (keep hashtags/mentions)\n",
    "    text = text.lower()\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    #text = re.sub(r\"\\.{2,}\", \".\", text)            # collapse dots\n",
    "    # space around punctuation for clearer tokenization\n",
    "    #text = re.sub(r\"([!?.,])\", r\" \\1 \", text)\n",
    "    #text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    text = expand_contractions(text)\n",
    "    text = replace_emojis(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    from nltk.corpus import stopwords\n",
    "    stopset = set(stopwords.words(\"english\"))\n",
    "    filtered = [w for w in words if w.lower() not in stopset]\n",
    "    return filtered\n",
    "\n",
    "def tokenize(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "# convert NLTK treebank tag to WordNet POS (for lemmatizer)\n",
    "from nltk.corpus import wordnet as wn\n",
    "def treebank_to_wordnet_pos(tb_tag):\n",
    "    if tb_tag.startswith(\"J\"):\n",
    "        return wn.ADJ\n",
    "    if tb_tag.startswith(\"V\"):\n",
    "        return wn.VERB\n",
    "    if tb_tag.startswith(\"N\"):\n",
    "        return wn.NOUN\n",
    "    if tb_tag.startswith(\"R\"):\n",
    "        return wn.ADV\n",
    "    return wn.NOUN\n",
    "\n",
    "def stem_words(words):\n",
    "    return {\n",
    "        \"porter\": [porter.stem(w) for w in words],\n",
    "        \"snowball\": [snow.stem(w) for w in words],\n",
    "        \"lancaster\": [lanc.stem(w) for w in words]\n",
    "    }\n",
    "\n",
    "def lemmatize_words(words, pos_tags):\n",
    "    # pos_tags: list of treebank tags from pos_tag\n",
    "    lemmas = []\n",
    "    for w, t in zip(words, pos_tags):\n",
    "        wnpos = treebank_to_wordnet_pos(t)\n",
    "        lemmas.append(wnl.lemmatize(w, wnpos))\n",
    "    return lemmas\n",
    "\n",
    "def chunk_pos_tags(pos_tags):\n",
    "    tree = chunker.parse(pos_tags)\n",
    "    return tree\n",
    "\n",
    "# very simple heuristic dependency extractor (not a real parser)\n",
    "def simple_dependencies(pos_tags):\n",
    "    words = [w for w,t in pos_tags]\n",
    "    tags = [t for w,t in pos_tags]\n",
    "    deps = []\n",
    "    # choose first verb as head (simple)\n",
    "    v_idx = next((i for i,t in enumerate(tags) if t.startswith(\"V\")), None)\n",
    "    if v_idx is None:\n",
    "        return deps\n",
    "    head = words[v_idx]\n",
    "    # subject: nearest noun/pronoun before verb\n",
    "    subj_idx = next((i for i in range(v_idx-1, -1, -1) if tags[i].startswith(\"N\") or tags[i] in (\"PRP\",\"NNP\")), None)\n",
    "    if subj_idx is not None:\n",
    "        deps.append((head, words[subj_idx], \"nsubj\"))\n",
    "    # direct object: first noun after verb\n",
    "    dobj_idx = next((i for i in range(v_idx+1, len(words)) if tags[i].startswith(\"N\")), None)\n",
    "    if dobj_idx is not None:\n",
    "        deps.append((head, words[dobj_idx], \"dobj\"))\n",
    "    # negation: 'not' or 'no' attaches to head\n",
    "    for i,w in enumerate(words):\n",
    "        if w.lower() in (\"not\",\"no\",\"n't\"):\n",
    "            deps.append((head, w, \"neg\"))\n",
    "    # adjectival modifiers attach to nearest noun\n",
    "    for i,(w,t) in enumerate(pos_tags):\n",
    "        if t.startswith(\"JJ\"):\n",
    "            # find nearest noun to right then left\n",
    "            target = None\n",
    "            for j in range(i+1, len(words)):\n",
    "                if tags[j].startswith(\"N\"):\n",
    "                    target = words[j]; break\n",
    "            if not target:\n",
    "                for j in range(i-1, -1, -1):\n",
    "                    if tags[j].startswith(\"N\"):\n",
    "                        target = words[j]; break\n",
    "            if target:\n",
    "                deps.append((target, w, \"amod\"))\n",
    "    # time words\n",
    "    for tword in (\"yesterday\",\"today\",\"tomorrow\",\"now\"):\n",
    "        if tword in [w.lower() for w in words]:\n",
    "            deps.append((head, tword, \"time\"))\n",
    "    return deps\n",
    "\n",
    "def agreement_check(pos_tags):\n",
    "    words = [w for w,t in pos_tags]; tags=[t for w,t in pos_tags]\n",
    "    v_idx = next((i for i,t in enumerate(tags) if t.startswith(\"V\")), None)\n",
    "    if v_idx is None:\n",
    "        return [\"No verb found\"]\n",
    "    subj_idx = next((i for i in range(v_idx-1, -1, -1) if tags[i].startswith(\"N\") or tags[i] in (\"PRP\",\"NNP\")), None)\n",
    "    if subj_idx is None:\n",
    "        return [\"No subject found\"]\n",
    "    subj = words[subj_idx]; subj_tag = tags[subj_idx]\n",
    "    verb = words[v_idx]; verb_tag = tags[v_idx]\n",
    "    # crude plural detection\n",
    "    subj_plural = subj_tag in (\"NNS\",\"NNPS\") or subj.lower().endswith(\"s\")\n",
    "    verb_3sg = verb_tag == \"VBZ\" or (verb.endswith(\"s\") and not verb.endswith(\"ss\"))\n",
    "    if subj_plural and verb_3sg:\n",
    "        return [f\"Mismatch: plural subject '{subj}' with 3rd-person-singular verb '{verb}'\"]\n",
    "    if (not subj_plural) and verb_3sg and subj.lower() in (\"i\",\"you\"):\n",
    "        return [f\"Possible mismatch with subject '{subj}' and verb '{verb}'\"]\n",
    "    return [\"Agreement heuristic OK\"]\n",
    "\n",
    "def simple_srl(deps, pos_tags):\n",
    "    roles = {}\n",
    "    for h,d,r in deps:\n",
    "        if r==\"nsubj\":\n",
    "            roles[\"agent\"]=d\n",
    "        elif r==\"dobj\":\n",
    "            # theme (object)\n",
    "            roles[\"theme\"]=d\n",
    "        elif r==\"iobj\":\n",
    "            roles[\"recipient\"]=d\n",
    "        elif r==\"neg\":\n",
    "            roles[\"polarity\"]=\"negative\"\n",
    "        elif r==\"amod\":\n",
    "            roles.setdefault(\"adjectives\", []).append(d)\n",
    "        elif r==\"time\":\n",
    "            roles[\"time\"]=d\n",
    "    return roles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "406c440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TWEET #1: Just finished reading an amazing book on AI ğŸ¤–ğŸ“š #MachineLearning #AI.\n",
      "\n",
      "1) Normalized ->\n",
      " just finished reading an amazing book on ai  robot  books  #machinelearning #ai \n",
      "\n",
      "2) Tokenized ->\n",
      " ['just', 'finished', 'reading', 'an', 'amazing', 'book', 'on', 'ai', 'robot', 'books', '#machinelearning', '#ai']\n",
      "\n",
      "3) Stopword filtered (keeps negations) ->\n",
      " ['finished', 'reading', 'amazing', 'book', 'ai', 'robot', 'books', '#machinelearning', '#ai']\n",
      "\n",
      "4) POS tags ->\n",
      " [('finished', 'VBN'), ('reading', 'NN'), ('amazing', 'VBG'), ('book', 'NN'), ('ai', 'VBP'), ('robot', 'NN'), ('books', 'NNS'), ('#machinelearning', 'VBG'), ('#ai', 'NN')]\n",
      "\n",
      "5) Stems ->\n",
      "\n",
      " Porter: ['finish', 'read', 'amaz', 'book', 'ai', 'robot', 'book', '#machinelearn', '#ai']\n",
      " Snowball: ['finish', 'read', 'amaz', 'book', 'ai', 'robot', 'book', '#machinelearn', '#ai']\n",
      " Lancaster: ['fin', 'read', 'amaz', 'book', 'ai', 'robot', 'book', '#machinelearning', '#ai']\n",
      "\n",
      "6) Lemmas ->\n",
      " ['finish', 'reading', 'amaze', 'book', 'ai', 'robot', 'book', '#machinelearning', '#ai']\n",
      "\n",
      "7) Chunk (tree) ->\n",
      " (S\n",
      "  finished/VBN\n",
      "  (NP reading/NN)\n",
      "  amazing/VBG\n",
      "  (NP book/NN)\n",
      "  ai/VBP\n",
      "  (NP robot/NN books/NNS)\n",
      "  (VP #machinelearning/VBG (NP #ai/NN)))\n",
      "\n",
      "8) Heuristic dependencies ->\n",
      " [('finished', 'reading', 'dobj')]\n",
      "\n",
      "9) Agreement check ->\n",
      " ['No subject found']\n",
      "\n",
      "10) Simple SRL ->\n",
      " {'theme': 'reading'}\n",
      "================================================================================\n",
      "TWEET #2: What a beautiful morning! â˜€ï¸ Feeling super motivated to start the day ğŸ’ª #GoodVibes\n",
      "\n",
      "1) Normalized ->\n",
      " what a beautiful morning   sun  feeling super motivated to start the day  strong  #goodvibes\n",
      "\n",
      "2) Tokenized ->\n",
      " ['what', 'a', 'beautiful', 'morning', 'sun', 'feeling', 'super', 'motivated', 'to', 'start', 'the', 'day', 'strong', '#goodvibes']\n",
      "\n",
      "3) Stopword filtered (keeps negations) ->\n",
      " ['beautiful', 'morning', 'sun', 'feeling', 'super', 'motivated', 'start', 'day', 'strong', '#goodvibes']\n",
      "\n",
      "4) POS tags ->\n",
      " [('beautiful', 'JJ'), ('morning', 'NN'), ('sun', 'NN'), ('feeling', 'VBG'), ('super', 'NN'), ('motivated', 'VBD'), ('start', 'JJ'), ('day', 'NN'), ('strong', 'JJ'), ('#goodvibes', 'NNS')]\n",
      "\n",
      "5) Stems ->\n",
      "\n",
      " Porter: ['beauti', 'morn', 'sun', 'feel', 'super', 'motiv', 'start', 'day', 'strong', '#goodvib']\n",
      " Snowball: ['beauti', 'morn', 'sun', 'feel', 'super', 'motiv', 'start', 'day', 'strong', '#goodvib']\n",
      " Lancaster: ['beauty', 'morn', 'sun', 'feel', 'sup', 'mot', 'start', 'day', 'strong', '#goodvibes']\n",
      "\n",
      "6) Lemmas ->\n",
      " ['beautiful', 'morning', 'sun', 'feel', 'super', 'motivate', 'start', 'day', 'strong', '#goodvibes']\n",
      "\n",
      "7) Chunk (tree) ->\n",
      " (S\n",
      "  (NP beautiful/JJ morning/NN sun/NN)\n",
      "  feeling/VBG\n",
      "  (NP super/NN)\n",
      "  (VP\n",
      "    motivated/VBD\n",
      "    (NP start/JJ day/NN)\n",
      "    (NP strong/JJ #goodvibes/NNS)))\n",
      "\n",
      "8) Heuristic dependencies ->\n",
      " [('feeling', 'sun', 'nsubj'), ('feeling', 'super', 'dobj'), ('morning', 'beautiful', 'amod'), ('day', 'start', 'amod'), ('#goodvibes', 'strong', 'amod')]\n",
      "\n",
      "9) Agreement check ->\n",
      " ['Agreement heuristic OK']\n",
      "\n",
      "10) Simple SRL ->\n",
      " {'agent': 'sun', 'theme': 'super', 'adjectives': ['beautiful', 'start', 'strong']}\n",
      "================================================================================\n",
      "TWEET #3: Ugh...@user my laptop crashed again right before my deadline ğŸ˜¤ #MondayBlues\n",
      "\n",
      "1) Normalized ->\n",
      " ugh   @user my laptop crashed again right before my deadline  annoyed  #mondayblues\n",
      "\n",
      "2) Tokenized ->\n",
      " ['ugh', '@user', 'my', 'laptop', 'crashed', 'again', 'right', 'before', 'my', 'deadline', 'annoyed', '#mondayblues']\n",
      "\n",
      "3) Stopword filtered (keeps negations) ->\n",
      " ['ugh', '@user', 'laptop', 'crashed', 'right', 'deadline', 'annoyed', '#mondayblues']\n",
      "\n",
      "4) POS tags ->\n",
      " [('ugh', 'IN'), ('@user', 'NNP'), ('laptop', 'NN'), ('crashed', 'VBD'), ('right', 'JJ'), ('deadline', 'NN'), ('annoyed', 'VBN'), ('#mondayblues', 'NNS')]\n",
      "\n",
      "5) Stems ->\n",
      "\n",
      " Porter: ['ugh', '@user', 'laptop', 'crash', 'right', 'deadlin', 'annoy', '#mondayblu']\n",
      " Snowball: ['ugh', '@user', 'laptop', 'crash', 'right', 'deadlin', 'annoy', '#mondayblu']\n",
      " Lancaster: ['ugh', '@user', 'laptop', 'crash', 'right', 'deadlin', 'annoy', '#mondayblues']\n",
      "\n",
      "6) Lemmas ->\n",
      " ['ugh', '@user', 'laptop', 'crash', 'right', 'deadline', 'annoy', '#mondayblues']\n",
      "\n",
      "7) Chunk (tree) ->\n",
      " (S\n",
      "  (PP ugh/IN (NP @user/NNP laptop/NN))\n",
      "  crashed/VBD\n",
      "  (NP right/JJ deadline/NN)\n",
      "  (VP annoyed/VBN (NP #mondayblues/NNS)))\n",
      "\n",
      "8) Heuristic dependencies ->\n",
      " [('crashed', 'laptop', 'nsubj'), ('crashed', 'deadline', 'dobj'), ('deadline', 'right', 'amod')]\n",
      "\n",
      "9) Agreement check ->\n",
      " ['Agreement heuristic OK']\n",
      "\n",
      "10) Simple SRL ->\n",
      " {'agent': 'laptop', 'theme': 'deadline', 'adjectives': ['right']}\n",
      "================================================================================\n",
      "TWEET #4: brb grabbing some â˜•ï¸ lol canâ€™t start work without caffeine ğŸ˜‚\n",
      "\n",
      "1) Normalized ->\n",
      " be right back grabbing some  coffee  laughing out loud can not start work without caffeine  laugh \n",
      "\n",
      "2) Tokenized ->\n",
      " ['be', 'right', 'back', 'grabbing', 'some', 'coffee', 'laughing', 'out', 'loud', 'can', 'not', 'start', 'work', 'without', 'caffeine', 'laugh']\n",
      "\n",
      "3) Stopword filtered (keeps negations) ->\n",
      " ['right', 'back', 'grabbing', 'coffee', 'laughing', 'loud', 'start', 'work', 'without', 'caffeine', 'laugh']\n",
      "\n",
      "4) POS tags ->\n",
      " [('right', 'RB'), ('back', 'RB'), ('grabbing', 'VBG'), ('coffee', 'NN'), ('laughing', 'VBG'), ('loud', 'JJ'), ('start', 'NN'), ('work', 'NN'), ('without', 'IN'), ('caffeine', 'NN'), ('laugh', 'NN')]\n",
      "\n",
      "5) Stems ->\n",
      "\n",
      " Porter: ['right', 'back', 'grab', 'coffe', 'laugh', 'loud', 'start', 'work', 'without', 'caffein', 'laugh']\n",
      " Snowball: ['right', 'back', 'grab', 'coffe', 'laugh', 'loud', 'start', 'work', 'without', 'caffein', 'laugh']\n",
      " Lancaster: ['right', 'back', 'grab', 'coff', 'laugh', 'loud', 'start', 'work', 'without', 'caffein', 'laugh']\n",
      "\n",
      "6) Lemmas ->\n",
      " ['right', 'back', 'grab', 'coffee', 'laugh', 'loud', 'start', 'work', 'without', 'caffeine', 'laugh']\n",
      "\n",
      "7) Chunk (tree) ->\n",
      " (S\n",
      "  right/RB\n",
      "  back/RB\n",
      "  grabbing/VBG\n",
      "  (NP coffee/NN)\n",
      "  (VP\n",
      "    laughing/VBG\n",
      "    (NP loud/JJ start/NN work/NN)\n",
      "    (PP without/IN (NP caffeine/NN laugh/NN))))\n",
      "\n",
      "8) Heuristic dependencies ->\n",
      " [('grabbing', 'coffee', 'dobj'), ('start', 'loud', 'amod')]\n",
      "\n",
      "9) Agreement check ->\n",
      " ['No subject found']\n",
      "\n",
      "10) Simple SRL ->\n",
      " {'theme': 'coffee', 'adjectives': ['loud']}\n",
      "\n",
      "Saved pipeline results -> tweet_pipeline_results.json\n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words(\"english\"))\n",
    "\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "    entry = {\"raw\": tweet}\n",
    "    # 1. normalization\n",
    "    norm = normalize_text(tweet)\n",
    "    entry[\"normalized\"] = norm\n",
    "    # 2. tokenization\n",
    "    toks = tokenize(norm)\n",
    "    entry[\"tokens\"] = toks\n",
    "    # 3. stopword filtering (but keep negations)\n",
    "    filtered = remove_stopwords(toks)\n",
    "    entry[\"filtered_tokens\"] = filtered\n",
    "    # 4. POS tagging\n",
    "    pos = pos_tag(filtered)\n",
    "    entry[\"pos_tags\"] = pos\n",
    "    # 5. stemming\n",
    "    stems = stem_words([w for w,t in pos])\n",
    "    entry[\"stems\"] = stems\n",
    "    # 6. lemmatization (wordnet)\n",
    "    lemmas = lemmatize_words([w for w,t in pos], [t for w,t in pos])\n",
    "    entry[\"lemmas\"] = lemmas\n",
    "    # 7. chunking\n",
    "    chunk_tree = chunk_pos_tags(pos)\n",
    "    entry[\"chunk_tree_str\"] = str(chunk_tree)\n",
    "    # 8. simple dependencies\n",
    "    deps = simple_dependencies(pos)\n",
    "    entry[\"dependencies\"] = deps\n",
    "    # 9. agreement check\n",
    "    entry[\"agreement\"] = agreement_check(pos)\n",
    "    # 10. simple SRL\n",
    "    entry[\"srl\"] = simple_srl(deps, pos)\n",
    "    results.append(entry)\n",
    "\n",
    "# Pretty print each stage per tweet:\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"TWEET #{i}: {r['raw']}\\n\")\n",
    "    print(\"1) Normalized ->\\n\", r[\"normalized\"])\n",
    "    print(\"\\n2) Tokenized ->\\n\", r[\"tokens\"])\n",
    "    print(\"\\n3) Stopword filtered (keeps negations) ->\\n\", r[\"filtered_tokens\"])\n",
    "    print(\"\\n4) POS tags ->\\n\", r[\"pos_tags\"])\n",
    "    print(\"\\n5) Stems ->\\n\")\n",
    "    print(\" Porter:\", r[\"stems\"][\"porter\"])\n",
    "    print(\" Snowball:\", r[\"stems\"][\"snowball\"])\n",
    "    print(\" Lancaster:\", r[\"stems\"][\"lancaster\"])\n",
    "    print(\"\\n6) Lemmas ->\\n\", r[\"lemmas\"])\n",
    "    print(\"\\n7) Chunk (tree) ->\\n\", r[\"chunk_tree_str\"])\n",
    "    print(\"\\n8) Heuristic dependencies ->\\n\", r[\"dependencies\"])\n",
    "    print(\"\\n9) Agreement check ->\\n\", r[\"agreement\"])\n",
    "    print(\"\\n10) Simple SRL ->\\n\", r[\"srl\"])\n",
    "\n",
    "# save results for later inspection\n",
    "with open(\"tweet_pipeline_results.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nSaved pipeline results -> tweet_pipeline_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d75d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72892f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
