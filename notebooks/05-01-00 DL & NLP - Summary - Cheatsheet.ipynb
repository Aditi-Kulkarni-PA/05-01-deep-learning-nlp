{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6fc2ae",
   "metadata": {},
   "source": [
    "# Summary - Cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4595b96",
   "metadata": {},
   "source": [
    "| **Aspect**                  | ğŸ§± **Lexical Processing**                                                                                        | ğŸ—ï¸ **Syntactic Processing**                                                                                        | ğŸ’¡ **Semantic Processing**                                                                                                                             |\n",
    "| --------------------------- | ---------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Level of Analysis**       | **Word-level**                                                                                                   | **Sentence-level (structure)**                                                                                      | **Meaning-level (concepts)**                                                                                                                           |\n",
    "| **Primary Focus**           | Deals with **individual words and their forms**                                                                  | Deals with **how words combine grammatically**                                                                      | Deals with **what the sentence actually means**                                                                                                        |\n",
    "| **Goal**                    | Clean, normalize, and prepare tokens for analysis                                                                | Identify **grammatical structure** and relationships                                                                | Extract **meaning, roles, and context**                                                                                                                |\n",
    "| **Order of Words**          | âŒ **Does NOT matter** (bag-of-words view)                                                                        | âœ… **Matters** (grammar depends on order)                                                                            | âœ… **Matters** (meaning depends on structure and context)                                                                                               |\n",
    "| **Main Operations / Tasks** | - Text normalization<br>- Tokenization<br>- Stopword removal<br>- Stemming / Lemmatization<br>- Spell correction | - POS tagging<br>- Chunking (NP, VP, PP)<br>- Constituency / Dependency parsing<br>- Grammatical agreement checking | - Semantic role labelling (who did what)<br>- Word sense disambiguation (WSD)<br>- Coreference resolution<br>- Semantic parsing<br>- Textual inference |\n",
    "| **Input**                   | Raw or cleaned text                                                                                              | Tokenized, tagged text                                                                                              | Parsed, structured text                                                                                                                                |\n",
    "| **Output**                  | Tokens, lemmas, normalized words                                                                                 | Parse trees, dependency graphs, POS tags                                                                            | Semantic frames, role graphs, meaning representations                                                                                                  |\n",
    "| **Representation**          | Words or morphemes                                                                                               | Phrases, grammatical links                                                                                          | Logical or conceptual relationships                                                                                                                    |\n",
    "| **Example Sentence**        | â€œThe cats are running.â€ â†’ `[the, cat, are, run]`                                                                 | [NP The cats] [VP are running]                                                                                      | agent(cats), action(run), time(now)                                                                                                                    |\n",
    "| **Knowledge Used**          | Morphological / lexical knowledge                                                                                | Grammatical / syntactic rules                                                                                       | Contextual / world knowledge                                                                                                                           |\n",
    "| **Common Tools**            | Tokenizers, stemmers, lemmatizers, spell checkers                                                                | POS taggers, CFG parsers, dependency parsers                                                                        | SRL systems, WordNet, BERT-based semantic models                                                                                                       |\n",
    "| **Type of Rules / Models**  | Rule-based or statistical (surface-level)                                                                        | Grammar-based or probabilistic (structural)                                                                         | Semantic / contextual (deep models)                                                                                                                    |\n",
    "| **Output Type**             | Clean tokens                                                                                                     | Structured syntax tree                                                                                              | Meaning graph / logical form                                                                                                                           |\n",
    "| **Typical Errors**          | Misspellings, inconsistent forms                                                                                 | Incorrect POS tags or parse trees                                                                                   | Misinterpreted context or ambiguity                                                                                                                    |\n",
    "| **Applications**            | Preprocessing, IR, indexing, text cleaning                                                                       | Grammar checking, translation alignment, syntax-based QA                                                            | Chatbots, question answering, summarization, semantic search                                                                                           |\n",
    "| **Example of Sensitivity**  | â€œcat dog runâ€ = â€œdog run catâ€ (same tokens)                                                                      | â€œdog chased catâ€ â‰  â€œcat chased dogâ€                                                                                 | â€œJohn gave Mary a giftâ€ â‰  â€œMary gave John a giftâ€                                                                                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c3caa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6557319",
   "metadata": {},
   "source": [
    "# Complete NLP Pipeline â€” Step-by-Step Transformation Table\n",
    "\n",
    "| **Stage**                      | **Process / Concept**                                 | **Description / Operation**                                                                                               | **Example Input â†’ Output**                                                                                        |\n",
    "| ------------------------------ | ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |\n",
    "| **1ï¸âƒ£ Lexical Processing**     | **Raw Input**                                         | Start with natural, noisy text (includes contractions, punctuation, emoji).                                               | â€œRiya didnâ€™t give her friend any gfits yesterday ğŸ˜Š.â€                                                             |\n",
    "|                                | **Text Normalisation**                                | Lowercasing, punctuation cleanup, encoding fix.                                                                           | â€œriya didnâ€™t give her friend any gfits yesterday ğŸ˜Šâ€                                                              |\n",
    "|                                | **Contraction Expansion**                             | Expand shortened forms (important for semantics).                                                                         | â€œriya did not give her friend any gfits yesterday ğŸ˜Šâ€                                                             |\n",
    "|                                | **Accent / Emoji Handling**                           | Replace emojis or diacritics with textual meaning.                                                                        | â€œriya did not give her friend any gfits yesterday smileâ€                                                          |\n",
    "|                                | **Tokenization**                                      | Split text into words/tokens.                                                                                             | [riya, did, not, give, her, friend, any, gfits, yesterday, smile]                                                 |\n",
    "|                                | **Stopword Removal**                                  | Remove common filler words (optional â€” keep â€œnotâ€ since it affects meaning).                                              | [riya, not, give, friend, gfits, yesterday, smile]                                                                |\n",
    "|                                | **Number / Symbol Handling**                          | Convert numbers/symbols to placeholders (not applicable here).                                                            | â€”                                                                                                                 |\n",
    "|                                | **Stemming / Lemmatization**                          | Convert to base dictionary forms.                                                                                         | [riya, not, give, friend, gfit, yesterday, smile]                                                                 |\n",
    "|                                | **Spell Correction (Edit Distance / Noisy Channel)**  | Fix possible misspellings using distance or probability models.                                                           | â€œgfitâ€ â†’ â€œgiftâ€ (if miswritten)                                                                                  |\n",
    "| **â†’ Output (Lexical Level)**   | Clean, base-level tokens ready for analysis.          | [riya, not, give, friend, gift, yesterday, smile]                                                                         |                                                                                                                   |\n",
    "| **2ï¸âƒ£ Syntactic Processing**   | **POS Tagging**                                       | Assign parts of speech to each token.                                                                                     | Riya/NNP, did/VBD, not/RB, give/VB, her/PRP$, friend/NN, any/DT, gifts/NNS, yesterday/NN, smile/NN                |\n",
    "|                                | **Chunking (Shallow Parsing)**                        | Group words into meaningful phrases (NP, VP, PP, ADVP).                                                                   | [NP Riya] [VP did not give] [NP her friend] [NP any gifts] [ADVP yesterday]                                       |\n",
    "|                                | **Constituency Parsing**                              | Build hierarchical tree (phrase-based).                                                                                   | [S [NP Riya] [VP did [RB not] [VP give [NP her friend] [NP any gifts]]] [ADVP yesterday]]                         |\n",
    "|                                | **Dependency Parsing**                                | Link headâ€“dependent pairs showing direct grammatical relations.                                                           | giveâ†’Riya (nsubj); giveâ†’friend (iobj); giveâ†’gifts (dobj); giveâ†’yesterday (advmod); didâ†’give (aux); giveâ†’not (neg) |\n",
    "|                                | **Grammatical Agreement Checking**                    | Ensure correct subjectâ€“verb, nounâ€“determiner, tense agreement.                                                            | âœ” â€œRiya did not giveâ€¦â€ (3rd person singular + auxiliary â€œdidâ€)                                                    |\n",
    "| **â†’ Output (Syntactic Level)** | Structured relations between words.                   | Head = â€œgiveâ€; Subject = â€œRiyaâ€; Negation = â€œnotâ€; Objects = â€œfriendâ€, â€œgiftsâ€; Modifier = â€œyesterdayâ€                    |                                                                                                                   |\n",
    "| **3ï¸âƒ£ Semantic Processing**    | **Semantic Role Labelling (SRL)**                     | Identify â€œwho did what to whom, when, and how.â€                                                                           | give(agent=Riya, recipient=friend, theme=gift, time=yesterday, polarity=negative)                                 |\n",
    "|                                | **Word Sense Disambiguation (WSD)**                   | Disambiguate polysemous words using context.                                                                              | â€œgiftâ€ â†’ physical present (not talent)                                                                            |\n",
    "|                                | **Coreference Resolution**                            | Link pronouns to entities.                                                                                                | her â†’ Riya                                                                                                        |\n",
    "|                                | **Semantic Parsing**                                  | Convert into logical / graph representation.                                                                              | NOT(give(Riya, friend, gift, yesterday))                                                                          |\n",
    "|                                | **Textual Inference (Optional)**                      | Check if one statement implies another.                                                                                   | â€œRiya didnâ€™t give any giftsâ€ â‡’ implies â€œRiya gave nothing.â€                                                       |\n",
    "|                                | **Sentiment / Emotion Detection (Extended Semantic)** | Interpret emotional cues from words or emoji.                                                                             | â€œsmileâ€ â†’ positive emotion, but negation â€œdid not giveâ€ â†’ neutral/mixed sentiment                                 |\n",
    "| **â†’ Output (Semantic Level)**  | Meaning representation (structured, interpretable).   | {action: give, agent: Riya, recipient: friend, object: gift, time: yesterday, polarity: negative, emotion: mild positive} |                                                                                                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add8e2f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722ded1",
   "metadata": {},
   "source": [
    "```perl\n",
    "NATURAL LANGUAGE PROCESSING (NLP)\n",
    "â”‚\n",
    "â”œâ”€â”€ 1ï¸âƒ£ LINGUISTIC FOUNDATIONS (Theoretical Base)\n",
    "â”‚   â”œâ”€ Phonology â†’ sound patterns & pronunciation\n",
    "â”‚   â”œâ”€ Morphology â†’ internal structure of words\n",
    "â”‚   â”œâ”€ Syntax â†’ grammatical arrangement of words\n",
    "â”‚   â”œâ”€ Semantics â†’ literal meaning of words/sentences\n",
    "â”‚   â””â”€ Pragmatics â†’ intended/contextual meaning\n",
    "â”‚\n",
    "â”œâ”€â”€ 2ï¸âƒ£ LEXICAL PROCESSING (Word-Level)\n",
    "â”‚   â”œâ”€ Text Normalization â†’ cleaning & consistency\n",
    "â”‚   â”‚   â”œâ”€ case conversion, punctuation, numbers\n",
    "â”‚   â”‚   â”œâ”€ contractions, slang, emojis\n",
    "â”‚   â”‚   â””â”€ encoding & whitespace fixes\n",
    "â”‚   â”œâ”€ Tokenization â†’ splitting text into units\n",
    "â”‚   â”‚   â”œâ”€ rule-based, statistical, or subword\n",
    "â”‚   â”‚   â””â”€ levels: word, subword, sentence, character\n",
    "â”‚   â”œâ”€ Stopword Removal â†’ filter common filler words\n",
    "â”‚   â”œâ”€ Morphological Analysis â†’ study of morphemes\n",
    "â”‚   â”‚   â”œâ”€ stemming & lemmatization\n",
    "â”‚   â”‚   â””â”€ normalization of word forms\n",
    "â”‚   â””â”€ String Similarity & Error Models\n",
    "â”‚       â”œâ”€ Edit Distance â†’ # of edits between strings\n",
    "â”‚       â””â”€ Noisy Channel Model â†’ P(w given x) correction\n",
    "â”‚\n",
    "â”œâ”€â”€ 3ï¸âƒ£ SYNTACTIC PROCESSING (Sentence-Level Structure)\n",
    "â”‚   â”œâ”€ POS Tagging â†’ assign grammatical categories\n",
    "â”‚   â”‚   â”œâ”€ rule-based, statistical (HMM), or Brill (hybrid)\n",
    "â”‚   â”œâ”€ Shallow Parsing (Chunking) â†’ detect phrases (NP, VP, PP)\n",
    "â”‚   â”œâ”€ Constituency Parsing â†’ build phrase-structure trees (CFG)\n",
    "â”‚   â”œâ”€ Dependency Parsing â†’ link words via headâ€“dependent relations\n",
    "â”‚   â”œâ”€ Deep Parsing â†’ combine syntax + semantics (who did what to whom)\n",
    "â”‚   â”œâ”€ Parse Tree vs Syntax Tree â†’ derivational vs abstract structure\n",
    "â”‚   â””â”€ Grammatical Agreement Checking â†’ subjectâ€“verb, pronounâ€“noun consistency\n",
    "â”‚\n",
    "â””â”€â”€ 4ï¸âƒ£ SEMANTIC PROCESSING (Meaning-Level)\n",
    "    â”œâ”€ Semantic Role Labelling (SRL) â†’ agent, theme, recipient, time\n",
    "    â”œâ”€ Word Sense Disambiguation (WSD) â†’ resolve polysemy (â€œbankâ€)\n",
    "    â”œâ”€ Semantic Parsing â†’ map sentences to logical/graph meaning\n",
    "    â”œâ”€ Coreference Resolution â†’ link pronouns to entities\n",
    "    â””â”€ Textual Inference & Meaning Representation\n",
    "        â”œâ”€ check entailment & implication (A â‡’ B)\n",
    "        â””â”€ use embeddings/logical forms for reasoning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1de5e",
   "metadata": {},
   "source": [
    "```perl\n",
    "Natural Language Processing (NLP) - LEXICAL PROCESSING\n",
    "â”‚\n",
    "â”œâ”€â”€ 1. Linguistic Foundations\n",
    "â”‚   â”œâ”€â”€ Phonology â€“ sound patterns and pronunciation (e.g., \"knight\" vs \"night\")\n",
    "â”‚   â”œâ”€â”€ Morphology â€“ internal structure of words (e.g., un + happy + ness â†’ unhappiness)\n",
    "â”‚   â”œâ”€â”€ Syntax â€“ grammatical arrangement (e.g., \"cat chased dog\" vs \"dog chased cat\")\n",
    "â”‚   â”œâ”€â”€ Semantics â€“ literal meaning of words/sentences (\"light\" â†’ illumination / not heavy)\n",
    "â”‚   â””â”€â”€ Pragmatics â€“ intended meaning in context (\"Can you pass the salt?\" â†’ request)\n",
    "â”‚\n",
    "â”œâ”€â”€ 2. NLP Applications\n",
    "â”‚   â”œâ”€â”€ Information Retrieval â€“ web search queries\n",
    "â”‚   â”œâ”€â”€ E-commerce â€“ product review analysis\n",
    "â”‚   â”œâ”€â”€ Customer Support â€“ chatbots\n",
    "â”‚   â”œâ”€â”€ Healthcare â€“ clinical note analysis\n",
    "â”‚   â”œâ”€â”€ Machine Translation â€“ e.g., Google Translate\n",
    "â”‚   â””â”€â”€ Voice Assistants â€“ Siri, Alexa, Google Assistant\n",
    "â”‚\n",
    "â”œâ”€â”€ 3. Text Normalization\n",
    "â”‚   â”œâ”€â”€ Basic Normalization Tasks\n",
    "â”‚   â”‚   â”œâ”€â”€ Case conversion â€“ â€œNatural Languageâ€ â†’ â€œnatural languageâ€\n",
    "â”‚   â”‚   â”œâ”€â”€ Punctuation & symbol handling â€“ â€œC++â€ â†’ keep â€œ++â€\n",
    "â”‚   â”‚   â”œâ”€â”€ Number standardization â€“ â€œten kgâ€ â†’ â€œ10 kgâ€\n",
    "â”‚   â”‚   â”œâ”€â”€ Contraction expansion â€“ â€œdonâ€™tâ€ â†’ â€œdo notâ€\n",
    "â”‚   â”‚   â”œâ”€â”€ Accent removal â€“ â€œrÃ©sumÃ©â€ â†’ â€œresumeâ€\n",
    "â”‚   â”‚   â””â”€â”€ Whitespace & encoding fixes â€“ standardize UTF-8, remove extra spaces\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ Advanced Normalization Tasks\n",
    "â”‚       â”œâ”€â”€ Slang/abbreviation expansion â€“ â€œbtwâ€ â†’ â€œby the wayâ€\n",
    "â”‚       â”œâ”€â”€ Unicode & emoji handling â€“ ğŸ˜Š â†’ â€œsmileâ€\n",
    "â”‚       â””â”€â”€ Text standardization pipelines â€“ regex/rule-based pipelines for canonical input\n",
    "â”‚\n",
    "â”œâ”€â”€ 4. Tokenization\n",
    "â”‚   â”œâ”€â”€ Levels\n",
    "â”‚   â”‚   â”œâ”€â”€ Multi-Word-level â€“ \"Combined words not to be broken\" â†’ [\"out-of-office\"]\n",
    "â”‚   â”‚   â”œâ”€â”€ Word-level â€“ â€œLanguage models are powerfulâ€ â†’ [language, models, are, powerful]\n",
    "â”‚   â”‚   â”œâ”€â”€ Subword-level â€“ â€œunhappinessâ€ â†’ [un, happy, ness]\n",
    "â”‚   â”‚   â”œâ”€â”€ Sentence-level â€“ split paragraphs into sentences\n",
    "â”‚   â”‚   â””â”€â”€ Character-level â€“ handles out of vocabulary words (OOV) â€œcatâ€ â†’ [c, a, t]\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ Approaches\n",
    "â”‚   â”‚   â”œâ”€â”€ Rule-based / Regex Tokenizers â€“ predefined delimiters; fast but brittle\n",
    "â”‚   â”‚   â”‚        NLTK s TreeBankWordTokenizer\n",
    "â”‚   â”‚   â”œâ”€â”€ Statistical Tokenizers â€“ learn boundaries (esp. for Chinese, Japanese)\n",
    "â”‚   â”‚   â”‚        Word-boundary prediction using character n-grams\n",
    "â”‚   â”‚   â”‚        Conditional Random Fields (CRFs) for Chinese segmentation.\n",
    "â”‚   â”‚   â””â”€â”€ Subword Algorithms â€“ Byte Pair Encoding (BPE), WordPiece (used in BERT, GPT)\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ Challenges\n",
    "â”‚       â”œâ”€â”€ Ambiguous boundaries (no spaces in Thai/Chinese)\n",
    "â”‚       â”œâ”€â”€ Multiword expressions (â€œNew York Cityâ€)\n",
    "â”‚       â”œâ”€â”€ Punctuation ambiguity (â€œU.S.â€ vs sentence end)\n",
    "â”‚       â”œâ”€â”€ Contractions (â€œdonâ€™tâ€ â†’ [do, not])\n",
    "â”‚       â”œâ”€â”€ URLs & emojis (<URL>, <EMOJI>)\n",
    "â”‚       â””â”€â”€ Importance â€“ controls vocabulary, improves model efficiency\n",
    "â”‚\n",
    "â”œâ”€â”€ 5. Stopword Removal\n",
    "â”‚   â”œâ”€â”€ Goal â€“ remove common words that add little meaning (â€œtheâ€, â€œisâ€, â€œofâ€)\n",
    "â”‚   â”œâ”€â”€ Benefits\n",
    "â”‚   â”‚   â”œâ”€â”€ Reduces dimensionality\n",
    "â”‚   â”‚   â”œâ”€â”€ Improves focus on meaningful words\n",
    "â”‚   â”‚   â”œâ”€â”€ Reduces noise in BoW/TF-IDF\n",
    "â”‚   â”‚   â””â”€â”€ Simplifies keyword matching in search\n",
    "â”‚   â”œâ”€â”€ Challenges\n",
    "â”‚   â”‚   â”œâ”€â”€ Task sensitivity â€“ â€œnotâ€, â€œneverâ€ important in sentiment analysis\n",
    "â”‚   â”‚   â”œâ”€â”€ Domain dependence â€“ generic lists may drop important domain words\n",
    "â”‚   â”‚   â””â”€â”€ Language dependence â€“ different stopword lists per language\n",
    "â”‚\n",
    "â”œâ”€â”€ 6. Morphological Analysis\n",
    "â”‚   â”œâ”€â”€ Morpheme Types\n",
    "â”‚   â”‚   â”œâ”€â”€ Free â€“ can stand alone (â€œbookâ€, â€œhappyâ€)\n",
    "â”‚   â”‚   â”œâ”€â”€ Bound â€“ prefixes/suffixes (â€œun-â€, â€œ-nessâ€)\n",
    "â”‚   â”‚   â”œâ”€â”€ Prefix â€“ before root (â€œunhappyâ€)\n",
    "â”‚   â”‚   â”œâ”€â”€ Suffix â€“ after root (â€œkindnessâ€)\n",
    "â”‚   â”‚   â”œâ”€â”€ Root/Stem â€“ main meaning carrier (â€œplayâ€)\n",
    "â”‚   â”‚   â””â”€â”€ Infix/Circumfix â€“ embedded or wrapped (rare in English)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ Functions in NLP\n",
    "â”‚   â”‚   â”œâ”€â”€ Vocabulary normalization â€“ groups inflected forms\n",
    "â”‚   â”‚   â”œâ”€â”€ POS tagging â€“ identifies tense, number, degree\n",
    "â”‚   â”‚   â”œâ”€â”€ Semantic clarity â€“ link related forms (create, creator, creation)\n",
    "â”‚   â”‚   â””â”€â”€ Downstream utility â€“ stemming, lemmatization, spell correction\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ Approaches\n",
    "â”‚   â”‚   â”œâ”€â”€ Rule-based morphological analyzers â€“ handcrafted linguistic rules\n",
    "â”‚   â”‚   â””â”€â”€ Statistical analyzers â€“ learned from corpora using CRFs/neural models\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ Applications\n",
    "â”‚       â”œâ”€â”€ Information Retrieval â€“ group â€œrunâ€, â€œrunningâ€, â€œranâ€\n",
    "â”‚       â”œâ”€â”€ Machine Translation â€“ maintain agreement\n",
    "â”‚       â”œâ”€â”€ Speech Recognition â€“ handle variants\n",
    "â”‚       â””â”€â”€ Text-to-Speech â€“ ensure correct pronunciation\n",
    "â”‚\n",
    "â”œâ”€â”€ 7. Stemming vs Lemmatization\n",
    "â”‚   â”œâ”€â”€ Stemming\n",
    "â”‚   â”‚   â”œâ”€â”€ Rule-based truncation of suffixes/prefixes (e.g., Porter Stemmer)\n",
    "â”‚   â”‚   â”œâ”€â”€ Output may be non-words (studies â†’ studi)\n",
    "â”‚   â”‚   â”œâ”€â”€ Fast but crude, language-specific\n",
    "â”‚   â”‚   â”œâ”€â”€ Common Algorithms:\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ Porter Stemmer (1980)\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ Snowball Stemmer (Porter2, 2001, multilingual, improved)\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ Lancaster Stemmer (more aggressive)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ Lemmatization\n",
    "â”‚   â”‚   â”œâ”€â”€ Linguistic normalization using POS and morphological analysis\n",
    "â”‚   â”‚   â”œâ”€â”€ Always produces valid dictionary words (studies â†’ study)\n",
    "â”‚   â”‚   â”œâ”€â”€ Handles irregulars (went â†’ go, better â†’ good)\n",
    "â”‚   â”‚   â”œâ”€â”€ Slower but more accurate\n",
    "â”‚   â”‚   â””â”€â”€ Uses WordNet, spaCy, or Stanza\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ Comparison Summary\n",
    "â”‚       â”œâ”€â”€ Stemming â†’ fast, rule-based, less accurate\n",
    "â”‚       â”œâ”€â”€ Lemmatization â†’ slower, semantic, more precise\n",
    "â”‚       â””â”€â”€ Use cases:\n",
    "â”‚           â”œâ”€â”€ Stemming â†’ search, topic modeling, IR\n",
    "â”‚           â””â”€â”€ Lemmatization â†’ semantic analysis, MT, QA\n",
    "â”‚\n",
    "â”œâ”€â”€ 8. String Similarity & Error Handling\n",
    "â”‚   â”œâ”€â”€ Edit Distance (Levenshtein Distance)\n",
    "â”‚   â”‚   â”œâ”€â”€ Measures minimal edits (insert, delete, substitute)\n",
    "â”‚   â”‚   â”œâ”€â”€ Example: kitten â†’ sitting = 3 edits\n",
    "â”‚   â”‚   â”œâ”€â”€ Matrix calculation\n",
    "â”‚   â”‚   â”‚        set first row and col as 0,1,2,3,4, ... \n",
    "â”‚   â”‚   â”‚        if (row == col) take diag number, if (row <> col) take min(trio)+1\n",
    "â”‚   â”‚   â”œâ”€â”€ Applications:\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ Spell correction\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ DNA sequence analysis\n",
    "â”‚   â”‚   â”‚   â”œâ”€â”€ Fuzzy search\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ Plagiarism detection\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ Noisy Channel Model\n",
    "â”‚       â”œâ”€â”€ Formula: P(w given x) âˆ P(x given w) Ã— P(w)\n",
    "â”‚       â”œâ”€â”€ Components:\n",
    "â”‚       â”‚   â”œâ”€â”€ Language Model (P(w)) â†’ how likely or common is the word \n",
    "â”‚       â”‚   â””â”€â”€ Error Model (P(x given w)) â†’ how likely is the typo given the word \n",
    "â”‚       â”œâ”€â”€ Example:\n",
    "â”‚       â”‚   â”œâ”€â”€ Observed: â€œspelingâ€\n",
    "â”‚       â”‚   â”œâ”€â”€ Candidates:\n",
    "â”‚       â”‚   â”‚   â€¢ spelling â†’ P(w)=0.0012, P(x|w)=0.5 â‡’ 0.0006 âœ…\n",
    "â”‚       â”‚   â”‚   â€¢ spieling â†’ 0.0001Ã—0.5=0.00005\n",
    "â”‚       â”‚   â”‚   â€¢ selling â†’ 0.0011Ã—0.05=0.000055\n",
    "â”‚       â”œâ”€â”€ Result: â€œspellingâ€ wins (most probable)\n",
    "â”‚       â”œâ”€â”€ Applications:\n",
    "â”‚       â”‚   â”œâ”€â”€ Spell correction\n",
    "â”‚       â”‚   â”œâ”€â”€ OCR & ASR error correction\n",
    "â”‚       â”‚   â”œâ”€â”€ Query auto-correction\n",
    "â”‚       â”‚   â””â”€â”€ Text normalization\n",
    "â”‚\n",
    "â””â”€â”€ 9. Summary of Preprocessing Pipeline\n",
    "â”‚   â”œâ”€â”€ Text Cleaning â†’ (normalize, handle noise)\n",
    "â”‚   â”œâ”€â”€ Tokenization â†’ (split text)\n",
    "â”‚   â”œâ”€â”€ Stopword Removal â†’ (filter common words)\n",
    "â”‚   â”œâ”€â”€ Morphological Processing â†’ (stem/lemma)\n",
    "â”‚   â”œâ”€â”€ String Similarity â†’ (detect/correct errors)\n",
    "â”‚   â””â”€â”€ Statistical Models â†’ (e.g., Noisy Channel, Language Models)\n",
    "â”‚\n",
    "â”‚\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234b73c",
   "metadata": {},
   "source": [
    "```perl\n",
    "SYNTACTIC PROCESSING  (Sentence-Level NLP)\n",
    "â”‚\n",
    "â”œâ”€â”€ 1ï¸âƒ£ POS TAGGING (Part-of-Speech Tagging)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Meaning: Assigns grammatical tags to each word.\n",
    "â”‚   â”œâ”€ Examples of common POS tags:\n",
    "â”‚   â”‚     â€¢ NN  = Noun (dog, city)\n",
    "â”‚   â”‚     â€¢ NNP = Proper Noun (India, Microsoft)\n",
    "â”‚   â”‚     â€¢ VB  = Verb (run, eat)\n",
    "â”‚   â”‚     â€¢ JJ  = Adjective (beautiful, quick)\n",
    "â”‚   â”‚     â€¢ RB  = Adverb (quickly)\n",
    "â”‚   â”‚     â€¢ PRP = Pronoun (he, they)\n",
    "â”‚   â”‚     â€¢ DT  = Determiner (the, a, this)\n",
    "â”‚   â”‚     â€¢ IN  = Preposition (in, on)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Approaches:\n",
    "â”‚   â”‚     â€¢ Rule-Based (lexicon + grammar rules)\n",
    "â”‚   â”‚     â€¢ Statistical (Hidden Markov Models â†’ learns P(tag|word), P(tag|prev_tag))\n",
    "â”‚   â”‚     â€¢ Transformation-Based (Brill Tagger)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Statistical Language Modeling:\n",
    "â”‚   â”‚     â€¢ n-gram Models:\n",
    "â”‚   â”‚         - Approximate sentence probability using (nâˆ’1)-history\n",
    "â”‚   â”‚         - Formula: P(w1â€¦wn) â‰ˆ Î  P(wi | w(iâˆ’n+1)â€¦w(iâˆ’1))\n",
    "â”‚   â”‚         - Examples: Unigram, Bigram, Trigram\n",
    "â”‚   â”‚\n",
    "â”‚   â”‚     â€¢ Laplace (Add-1) Smoothing:\n",
    "â”‚   â”‚         - Avoids zero probabilities for unseen n-grams\n",
    "â”‚   â”‚         - Formula:  P(wi|wi-1) = (count + 1) / (total + V)\n",
    "â”‚   â”‚         - V = vocabulary size\n",
    "â”‚   â”‚\n",
    "â”‚   â”‚     â€¢ Perplexity (PP):\n",
    "â”‚   â”‚         - Measures how well an LM predicts test data\n",
    "â”‚   â”‚         - Formula: PP = 2^(âˆ’ (1/N) Î£ log2 P(sentence))\n",
    "â”‚   â”‚         - Interpretation: lower PP â†’ better model\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€ Example:\n",
    "â”‚         Sentence: \"The quick fox jumps\"\n",
    "â”‚         Tags:      DT   JJ    NN  VBZ\n",
    "â”‚\n",
    "â”œâ”€â”€ 2ï¸âƒ£ SHALLOW PARSING (CHUNKING)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Meaning: Identifies phrase-level chunks: \n",
    "â”‚   â”‚     Only top level phrases - NP, VP, PP, etc. No internal structure. No hierarchical tree. No dependency arcs.\n",
    "â”‚   â”‚     Example (Chunking)\n",
    "â”‚   â”‚     Sentence: The quick brown fox jumps over the lazy dog\n",
    "â”‚   â”‚     Chunking: [NP The   quick brown fox]  [VP jumps] [PP over] [NP the  lazy dog]\n",
    "â”‚   â”‚     IOB Tag : [   B-NP  I-NP  I-NP  I-NP] [   B-VP ] [   B-PP] [   B-NP I-NP I-NP]\n",
    "â”‚   â”‚     \n",
    "â”‚   â”œâ”€ IOB Tagging (Insideâ€“Outsideâ€“Beginning):\n",
    "â”‚   â”‚     â€¢ B-NP = Beginning of Noun Phrase\n",
    "â”‚   â”‚     â€¢ I-NP = Inside Noun Phrase\n",
    "â”‚   â”‚     â€¢ O    = Outside any chunk\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Example (NER): uses entity-type labels Location, Person, Organization (LOC, PER, ORG)\n",
    "â”‚   â”‚     Sentence: \"The city New York is crowded.\"\n",
    "â”‚   â”‚     Named Entity: \"New York\" (a proper noun / location)\n",
    "â”‚   â”‚     IOB:\n",
    "â”‚   â”‚       The      â†’ O\n",
    "â”‚   â”‚       city     â†’ O\n",
    "â”‚   â”‚       New      â†’ B-LOC\n",
    "â”‚   â”‚       York     â†’ I-LOC\n",
    "â”‚   â”‚       is       â†’ O\n",
    "â”‚   â”‚       crowded  â†’ O\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€ Used for:\n",
    "â”‚         â€¢ NER (Named Entity Recognition)\n",
    "â”‚         â€¢ Relation Extraction\n",
    "â”‚\n",
    "â”œâ”€â”€ 3ï¸âƒ£ CONSTITUENCY PARSING (full phrase-structure tree â†’ parse tree / syntax tree) \n",
    "â”‚   â”œâ”€ Meaning: Builds hierarchical phrase structure using Context-Free Grammar (CFG) rules.\n",
    "â”‚\n",
    "â”‚   â”œâ”€ PARSE TREE vs SYNTAX TREE\n",
    "â”‚   â”‚   Parse Tree: Shows full CFG derivation steps.\n",
    "â”‚   â”‚   Syntax Tree: Simplified final grammatical structure.\n",
    "â”‚   â”‚   Relationship: Syntax Tree is abstracted form of Parse Tree.\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Example: \n",
    "â”‚   â”‚    Sentence: \"The quick brown fox jumps over the lazy dog\"\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€   Syntax Tree (Simplified Structure): \n",
    "â”‚   â”‚    Only shows NP, VP, PP hierarchy. Words are grouped as phrases instead of token-level tags\n",
    "â”‚   â”‚             [S \n",
    "â”‚   â”‚                [NP The quick brown fox] \n",
    "â”‚   â”‚                [VP jumps \n",
    "â”‚   â”‚                    [PP over \n",
    "â”‚   â”‚                        [NP the lazy dog]\n",
    "â”‚   â”‚             ]]\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€   Parse Tree (Phrase-Structure Parsing): Full Grammar, NP â†’ DT JJ JJ NN, PP â†’ IN NP, VP â†’ VBZ PP\n",
    "â”‚   â”‚     [S \n",
    "â”‚   â”‚        [NP [DT The] [JJ quick] [JJ brown] [NN fox] ]\n",
    "â”‚   â”‚        [VP jumps \n",
    "â”‚   â”‚            [PP over \n",
    "â”‚   â”‚                [NP [DT the] [JJ lazy] [NN dog]]\n",
    "â”‚   â”‚        ]\n",
    "â”‚   â””â”€     ]\n",
    "â”‚\n",
    "â”œâ”€â”€ 4ï¸âƒ£ DEPENDENCY PARSING\n",
    "â”‚   â”œâ”€ Meaning: Represents grammar using relations between head â†’ dependent words.\n",
    "â”‚   â”œâ”€ Example:\n",
    "â”‚   â”‚     Sentence: \"The fox chased the rabbit.\"\n",
    "â”‚   â”‚     Dependencies:\n",
    "â”‚   â”‚       chased â†’ fox     (nsubj = nominal subject)\n",
    "â”‚   â”‚       chased â†’ rabbit  (obj   = direct object)\n",
    "â”‚   â”‚       rabbit â†’ the     (det   = determiner)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Methods:\n",
    "â”‚   â”‚     â€¢ Rule-Based\n",
    "â”‚   â”‚     â€¢ Transition-Based (shiftâ€“reduce)\n",
    "â”‚   â”‚     â€¢ Graph-Based\n",
    "â”‚   â”‚     â€¢ Neural (spaCy, BERT parsers)\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€ Used for:\n",
    "â”‚         â€¢ Information Extraction\n",
    "â”‚         â€¢ Semantic Parsing\n",
    "â”‚\n",
    "â”œâ”€â”€ 5ï¸âƒ£ DEEP PARSING / SEMANTIC ROLE LABELING (SRL)\n",
    "â”‚   â”œâ”€ Meaning: Goes beyond syntax â†’ assigns semantic roles (who did what, to whom, when, where).\n",
    "â”‚   â”œâ”€ Example:\n",
    "â”‚   â”‚     Sentence: \"Riya gave her friend a gift yesterday.\"\n",
    "â”‚   â”‚     Roles:\n",
    "â”‚   â”‚       â€¢ Agent      = Riya\n",
    "â”‚   â”‚       â€¢ Recipient  = her friend\n",
    "â”‚   â”‚       â€¢ Theme      = gift\n",
    "â”‚   â”‚       â€¢ Time       = yesterday\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Note:\n",
    "â”‚   â”‚     *Although it is semantic, most NLP courses place it under\n",
    "â”‚   â”‚     deep parsing / advanced parsing within syntactic processing.*\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€ Used for: QA, Chatbots, Translation, Information Extraction\n",
    "â”‚\n",
    "â””â”€â”€ 6ï¸âƒ£ GRAMMATICAL AGREEMENT CHECKING\n",
    "    â”‚\n",
    "    â”œâ”€ Checks:\n",
    "    â”‚     â€¢ Number (Singular/ Plural)\n",
    "    â”‚     â€¢ Person (1st, 2nd, 3rd)\n",
    "    â”‚     â€¢ Gender (Masculine/ Feminine/ Neutral)\n",
    "    â”‚     â€¢ Case (Nominative / Objective / Possessive)\n",
    "    â”‚     â€¢ Tense (Past / Present / Future)\n",
    "    â”‚\n",
    "    â”œâ”€ Types:\n",
    "    â”‚     â€¢ Subjectâ€“Verb (She runs / âœ˜ She run)\n",
    "    â”‚     â€¢ Pronounâ€“Noun (The boy lost his book / âœ˜ their book)\n",
    "    â”‚     â€¢ Determinerâ€“Noun (These apples / âœ˜ This apples)\n",
    "    â”‚\n",
    "    â”œâ”€ Template-Based Text Generation:\n",
    "    â”‚       â€¢ â€œfill-the-slotsâ€ rules: \n",
    "    â”‚         Example: \"The <ADJ> <NOUN> <VERB> <NOUN>\"\n",
    "    â”‚       â€¢ Used in early chatbots & rule-based NLG systems\n",
    "    â”‚\n",
    "    â”œâ”€ Rule-Based Machine Translation / SMT:\n",
    "    â”‚       â€¢ Early NLP translation â†’ hand-coded grammar rules\n",
    "    â”‚       â€¢ SMT (Statistical Machine Translation): uses\n",
    "    â”‚         phrase tables, alignment models (IBM Models), n-grams\n",
    "    â”‚       â€¢ Probability: P(target|source)\n",
    "    â”‚\n",
    "    â””â”€ Used for:\n",
    "          â€¢ Grammar checkers\n",
    "          â€¢ Translation\n",
    "          â€¢ Education tools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0304a",
   "metadata": {},
   "source": [
    "```perl\n",
    "| NLP \n",
    "â”‚\n",
    "â””â”€â”€ 3ï¸âƒ£ SEMANTIC PROCESSING (Meaning-Level)\n",
    "    â”‚\n",
    "    â”œâ”€ ğŸ”¹ Word Sense Disambiguation (WSD)\n",
    "    â”‚   â”œâ”€ Resolves ambiguous/polysemous words using context \n",
    "    â”‚   â”‚     e.g., â€œbankâ€ â†’ riverbank vs. financial institution\n",
    "    â”‚   â”œâ”€ Uses â†’ Context words, corpora, WordNet, embeddings\n",
    "    â”‚   â””â”€ Use â†’ Translation, QA, IR, Search\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ **Techniques**\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚  1ï¸âƒ£ Lesk Algorithm (Dictionary / Gloss-based)\n",
    "    â”‚   â”‚      â€¢ Match overlap between gloss definitions and context words  \n",
    "    â”‚   â”‚      â€¢ Example: â€œShe sat by the bank.â€ â†’ riverbank (overlap: â€œwaterâ€, â€œshoreâ€)\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚  2ï¸âƒ£ Knowledge-Based WSD\n",
    "    â”‚   â”‚      â€¢ Uses WordNet / ConceptNet  \n",
    "    â”‚   â”‚      â€¢ Path similarity between synsets  \n",
    "    â”‚   â”‚      â€¢ Example: context = â€œmoney, depositâ€ â†’ closer to financial-bank synset\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚  3ï¸âƒ£ Distributional WSD (Co-occurrence-based)  \n",
    "    â”‚   â”‚      â€¢ Meaning = words with similar contexts have similar meanings  \n",
    "    â”‚   â”‚      â€¢ Build Co-occurrence Matrix  \n",
    "    â”‚   â”‚         - Rows = target words  \n",
    "    â”‚   â”‚         - Columns = context words  \n",
    "    â”‚   â”‚         - Entries = counts / frequencies  \n",
    "    â”‚   â”‚      â€¢ Use Pointwise Mutual Information (PMI) / Positive-PMI (PPMI)  \n",
    "    â”‚   â”‚         - PMI = association strength between word & context  â€‹PMI(w,c) = log2 [â€‹P(w)P(c) / P(w,c)]\n",
    "    â”‚   â”‚         - PPMI = positive PMI (removes negative noise)  \n",
    "    â”‚   â”‚      â€¢ Disambiguate by comparing context similarity  \n",
    "    â”‚   â”‚      â€¢ Example:  \n",
    "    â”‚   â”‚         Sentence = â€œShe sat by the bank.â€  \n",
    "    â”‚   â”‚         Context words = sat, river â†’ high PPMI with riverbank sense\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚  4ï¸âƒ£ Embedding-based (Vector Similarity) \n",
    "    â”‚   â”‚      â€¢ Use word embeddings (Word2Vec / GloVe / BERT)  \n",
    "    â”‚   â”‚      â€¢ Sense = closest vector cluster to context  \n",
    "    â”‚   â”‚      â€¢ Cosine similarity to pick correct sense  \n",
    "    â”‚   â”‚      â€¢ Example:  \n",
    "    â”‚   â”‚         â€œHe waited at the bank.â€ â†’ context vector near *finance embeddings*\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚  5ï¸âƒ£ Supervised ML WSD\n",
    "    â”‚   â”‚      â€¢ Train classifier on labeled sense data  \n",
    "    â”‚   â”‚      â€¢ Features: surrounding words, POS tags, dependency links\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Example A â€” Ambiguous \"plant\"\n",
    "    â”‚   â”‚     - Sentence: â€œThe workers cleaned the plant after the shift.â€\n",
    "    â”‚   â”‚     - Lesk: overlaps with gloss(factory) â†’ choose factory  \n",
    "    â”‚   â”‚     - WordNet: context (workers, shift) â†’ close to industry synset  \n",
    "    â”‚   â”‚     - Distributional/PPMI: context frequently associated with factory  \n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Example B â€” Ambiguous â€œbankâ€\n",
    "    â”‚   â”‚     - S1: â€œShe deposited money at the bank.â€ â†’ financial-bank  \n",
    "    â”‚   â”‚     - S2: â€œHe sat on the bank and fished.â€ â†’ riverbank\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59faae19",
   "metadata": {},
   "source": [
    "```perl\n",
    "    â”œâ”€ ğŸ”¹ Semantic Role Labelling (SRL)\n",
    "    â”‚   â”œâ”€ Meaning: Identifies \"who did what to whom, when, where, how\".\n",
    "    â”‚   â”œâ”€ SRL = semantic equivalent of POS + constituency â†’ extracts meaning.\n",
    "    â”‚   â”œâ”€ Connects syntax â†’ semantics.\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Example\n",
    "    â”‚   â”‚     Sentence: \"Riya gave her friend a gift yesterday in school.\"\n",
    "    â”‚   â”‚     Roles:\n",
    "    â”‚   â”‚       â€¢ Agent (ARG0)     = Riya\n",
    "    â”‚   â”‚       â€¢ Recipient (ARG2) = her friend\n",
    "    â”‚   â”‚       â€¢ Theme (ARG1)     = a gift\n",
    "    â”‚   â”‚       â€¢ Time (ARGM-TMP)  = yesterday\n",
    "    â”‚   â”‚       â€¢ Location (ARGM-LOC) = in school\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Core Roles (PropBank ARG0â€“ARG5)\n",
    "    â”‚   â”‚     â€¢ ARG0 â†’ Agent / Doer / Causer - â€œJohn opened the door.â€ â†’ ARG0 = John\n",
    "    â”‚   â”‚     â€¢ ARG1 â†’ Theme / Patient / Thing acted upon - â€œJohn opened the door.â€ â†’ ARG1 = the door\n",
    "    â”‚   â”‚     â€¢ ARG2 â†’ Recipient / Beneficiary / End point - â€œRiya gave her friend a gift.â€ â†’ ARG2 = her friend\n",
    "    â”‚   â”‚     â€¢ ARG3 â†’ Start point / Source - â€œRiya took the book from the shelf.â€ â†’ ARG3 = the shelf\n",
    "    â”‚   â”‚     â€¢ ARG4 â†’ Destination / End point (alternate) - â€œShe delivered the package to Mumbai.â€ â†’ ARG4 = to Mumbai\n",
    "    â”‚   â”‚     â€¢ ARG5 â†’ Direction / Purpose (rare) - â€œHe loaded the truck with hay.â€ â†’ ARG5 = with hay\n",
    "    â”‚   â”œâ”€ Adjunct (Modifier) Roles (ARGM-X)\n",
    "    â”‚   â”‚     â€¢ ARGM-TMP â†’ Time - â€œShe left yesterday.â€ â†’ yesterday\n",
    "    â”‚   â”‚     â€¢ ARGM-LOC â†’ Location - â€œHe slept in school.â€ â†’ in school\n",
    "    â”‚   â”‚     â€¢ ARGM-MNR â†’ Manner (how) - â€œShe spoke softly.â€ â†’ softly\n",
    "    â”‚   â”‚     â€¢ ARGM-CAU â†’ Cause - â€œHe cried because of pain.â€ â†’ because of pain\n",
    "    â”‚   â”‚     â€¢ ARGM-DIR â†’ Direction   - â€œShe ran toward the gate.â€ â†’ toward the gate\n",
    "    â”‚   â”‚     â€¢ ARGM-PRP â†’ Purpose  - â€œHe studied to pass the exam.â€ â†’ to pass the exam\n",
    "    â”‚   â”‚     â€¢ ARGM-ADV â†’ Adverbial (general)  - â€œUnfortunately, he lost.â€ â†’ unfortunately\n",
    "    â”‚   â”‚     â€¢ ARGM-NEG â†’ Negation   - â€œHe did not go.â€ â†’ not\n",
    "    â”‚   â”‚     â€¢ ARGM-MOD â†’ Modal  - â€œShe will go.â€ â†’ will\n",
    "    â”‚   â”‚     â€¢ ARGM-EXT â†’ Extent / Degree  - â€œShe increased the speed by 20%.â€ â†’ by 20%\n",
    "    â”‚   â”‚     â€¢ ARGM-COM â†’ Comitative (with whom)  - â€œHe walked with his friend.â€ â†’ with his friend\n",
    "    â”‚\n",
    "    â””â”€   Summary:\n",
    "            â€¢ ARG0â€“ARG5 â†’ Core semantic arguments (predicate-specific)\n",
    "            â€¢ ARGM-X â†’ Adjunct roles (time, place, manner, purpose, etc.)\n",
    "            â€¢ SRL explains meaning structure beyond grammar\n",
    "            â€¢ Used in QA, chatbots, translation, IE, summarization\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465318e",
   "metadata": {},
   "source": [
    "```perl\n",
    "    â”œâ”€ ğŸ”¹ Named Entity Recognition (NER)\n",
    "    â”‚   â”œâ”€ Meaning:\n",
    "    â”‚   â”‚     Identifies and classifies real-world entities in text \n",
    "    â”‚   â”‚     (persons, locations, organizations, dates, etc.)\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Common Entity Types:\n",
    "    â”‚   â”‚     â€¢ PERSON   â€“ names of people (Barack Obama)\n",
    "    â”‚   â”‚     â€¢ ORG      â€“ organizations (Microsoft, UN)\n",
    "    â”‚   â”‚     â€¢ LOC      â€“ physical locations (Paris, Himalayas)\n",
    "    â”‚   â”‚     â€¢ GPE      â€“ geopolitical entities (India, Europe)\n",
    "    â”‚   â”‚     â€¢ DATE     â€“ dates (21 June, yesterday)\n",
    "    â”‚   â”‚     â€¢ TIME     â€“ time expressions (5 PM)\n",
    "    â”‚   â”‚     â€¢ MONEY    â€“ â‚¹500, $20\n",
    "    â”‚   â”‚     â€¢ PRODUCT  â€“ iPhone, Tesla Model S\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Approaches:\n",
    "    â”‚   â”‚     â€¢ Rule-Based NER\n",
    "    â”‚   â”‚         - Uses dictionaries + handcrafted rules\n",
    "    â”‚   â”‚         - e.g., â€œDr.â€ before name â†’ PERSON\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚     â€¢ Statistical NER\n",
    "    â”‚   â”‚         - Hidden Markov Models (HMMs) - Feature based learning\n",
    "    â”‚   â”‚         - Conditional Random Fields (CRF) - Feature based learning\n",
    "    â”‚   â”‚         - Learns sequence probability P(tags | words)\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚     â€¢ Neural NER\n",
    "    â”‚   â”‚         - BiLSTM + CRF (classic state-of-the-art pre-BERT)\n",
    "    â”‚   â”‚         - Transformers (BERT, RoBERTa, GPT models)\n",
    "    â”‚   â”‚         - Learns contextual meaning â†’ best accuracy\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Conditional Random Fields (CRF) for NER\n",
    "    â”‚   â”‚   â”œâ”€ Meaning:\n",
    "    â”‚   â”‚     â€¢ CRF = Conditional Random Field\n",
    "    â”‚   â”‚     â€¢ A probabilistic sequence-labeling model used to predict tag for each word\n",
    "    â”‚   â”‚     â€¢ Considers entire sentence context (unlike per-token classifiers)\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚   â”œâ”€ Why CRF is used in NER:\n",
    "    â”‚   â”‚     â€¢ Enforces valid label transitions (e.g., I-PER cannot follow B-LOC)\n",
    "    â”‚   â”‚     â€¢ Captures dependencies between neighboring words/tags\n",
    "    â”‚   â”‚     â€¢ More accurate than simple classifiers (HMM, Maximum Entropy)\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚   â”œâ”€ What CRF learns from:\n",
    "    â”‚   â”‚     â€¢ Word features (word itself, lowercase, suffix, prefix)\n",
    "    â”‚   â”‚     â€¢ POS tags\n",
    "    â”‚   â”‚     â€¢ Capitalization pattern\n",
    "    â”‚   â”‚     â€¢ Previous + next words\n",
    "    â”‚   â”‚     â€¢ Previous predicted tags\n",
    "    â”‚   â”‚         \n",
    "    â”‚   â”œâ”€ IOB / BIO Tagging for NER:\n",
    "    â”‚   â”‚     B-XXX = Beginning of entity\n",
    "    â”‚   â”‚     I-XXX = Inside entity\n",
    "    â”‚   â”‚     O     = Outside entity\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Example (IOB tagging):\n",
    "    â”‚   â”‚     Sentence: \"Barack Obama lives in Washington D.C.\"\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚     Tokens:   Barack   Obama   lives   in   Washington   D.C   .\n",
    "    â”‚   â”‚     Labels:   B-PER   I-PER    O      O    B-GPE       I-GPE  O\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”‚     Explanation:\n",
    "    â”‚   â”‚         Barack Obama     â†’ Person (PER)\n",
    "    â”‚   â”‚         Washington D.C.  â†’ Geo-Political Entity (GPE)    \n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Features typically used (esp. in CRF/HMM systems):\n",
    "    â”‚   â”‚     â€¢ Current word\n",
    "    â”‚   â”‚     â€¢ Word shape: Aa, AA, aa, digits\n",
    "    â”‚   â”‚     â€¢ Prefixes & suffixes (e.g., â€œ-stanâ€, â€œMc-â€, â€œdr-â€)\n",
    "    â”‚   â”‚     â€¢ POS tags (NNP â†’ common signal for proper nouns)\n",
    "    â”‚   â”‚     â€¢ Context words (window = Â±2 words) \n",
    "    â”‚   â”‚     â€¢ Capitalization patterns\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€ Why NER is SEMANTIC (not syntactic):\n",
    "    â”‚   â”‚     â€¢ Syntactic structure alone canâ€™t tell what is a real-world entity.\n",
    "    â”‚   â”‚       Example: â€œApple is hiring.â€ â†’ NP, but Apple = ORG (semantic)\n",
    "    â”‚   â”‚     â€¢ Requires world knowledge + meaning + context\n",
    "    â”‚   â”‚     â€¢ Uses syntax as a helper (POS, chunks), not the main goal\n",
    "    â”‚   â”‚\n",
    "    â”‚   â””â”€ Used for:\n",
    "    â”‚         â€¢ Information extraction\n",
    "    â”‚         â€¢ Search engines (query understanding)\n",
    "    â”‚         â€¢ Chatbots / Assistants\n",
    "    â”‚         â€¢ Document understanding\n",
    "    â”‚         â€¢ Financial / Legal text mining\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104eb4f",
   "metadata": {},
   "source": [
    "```perl\n",
    "â”œâ”€â”€ ğŸ”¹ Coreference Resolution\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Meaning:\n",
    "â”‚   â”‚     Identify when multiple expressions refer to the same real-world entity.\n",
    "â”‚   â”‚     Forms \"coreference chains\" linking mentions together.\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Why it matters:\n",
    "â”‚   â”‚     Essential for understanding narratives, summaries, QA, chatbots,\n",
    "â”‚   â”‚     dialogue continuity, and reading comprehension.\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Types of Coreference:\n",
    "â”‚   â”‚   â€¢ Pronoun Coreference - â€œRiya lost her pen.â€ â†’ her â†’ Riya\n",
    "â”‚   â”‚   â€¢ Noun Phrase Coreference - â€œRiya met a girl. The girl smiled.â€ â†’ girl â†’ the girl\n",
    "â”‚   â”‚   â€¢ Appositive Coreference  - â€œRiya, the manager, approved the request.â€  â†’ Riya = the manager\n",
    "â”‚   â”‚   â€¢ Demonstrative Coreference  - â€œI found a car. That vehicle was cheap.â€ â†’ car = that vehicle\n",
    "â”‚   â”‚   â€¢ Anaphora (back-reference)  - noun introduced first â†’ pronoun refers back - â€œJohn arrived. He was tired.â€ â†’ He â†’ John\n",
    "â”‚   â”‚   â€¢ Cataphora (forward reference) - pronoun appears first, noun later\n",
    "â”‚   â”‚      - â€œWhen she arrived, Riya was exhausted.â€ â†’ she â†’ Riya\n",
    "â”‚   â”‚   â€¢ Bridging / Associative Coreference - implied connection, not identical mentions \n",
    "â”‚   â”‚      - â€œJohn entered a house. The roof was leaking.â€ \n",
    "â”‚   â”‚      â†’ roof is part of house (not same referent, but linked)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Outputs:\n",
    "â”‚   â”‚     â€¢ Coreference Chains (entity clusters)\n",
    "â”‚   â”‚     Example:\n",
    "â”‚   â”‚         Sentence: â€œRiya gave her friend a gift. She wrapped it herself.â€\n",
    "â”‚   â”‚         Chains:\n",
    "â”‚   â”‚              Chain 1: Riya â†’ She\n",
    "â”‚   â”‚              Chain 2: gift â†’ it\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Approaches & Models:\n",
    "â”‚   â”‚     â€¢ Rule-Based (Hobbs Algorithm, Lappin & Leass)\n",
    "â”‚   â”‚     â€¢ Statistical ML (mention-pair models, clustering)\n",
    "â”‚   â”‚     â€¢ Feature-Based ML (SVM, MaxEnt classifiers)\n",
    "â”‚   â”‚     â€¢ Neural Coreference Models\n",
    "â”‚   â”‚         - end-to-end models (e.g., Lee et al.)\n",
    "â”‚   â”‚         - Transformers (BERT-based, SpanBERT, Longformer)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Features used in Coreference:\n",
    "â”‚   â”‚     â€¢ Gender agreement       (he â†” John)\n",
    "â”‚   â”‚     â€¢ Number agreement       (they â†” children)\n",
    "â”‚   â”‚     â€¢ Semantic compatibility (dog â†” animal)\n",
    "â”‚   â”‚     â€¢ Syntactic constraints  (subject/object)\n",
    "â”‚   â”‚     â€¢ Distance between mentions\n",
    "â”‚   â”‚     â€¢ Contextual embeddings (BERT)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Challenges:\n",
    "â”‚   â”‚     â€¢ Ambiguous pronouns (â€œRiya spoke to Anu. She was upset.â€)\n",
    "â”‚   â”‚     â€¢ Long-distance references\n",
    "â”‚   â”‚     â€¢ World knowledge required (â€œThe car hit the pole. The driver was hurt.â€)\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€ Applications:\n",
    "â”‚   â”‚      â€¢ Text summarization\n",
    "â”‚   â”‚      â€¢ Question answering\n",
    "â”‚   â”‚      â€¢ Chatbots and dialogue systems\n",
    "â”‚   â”‚      â€¢ Reading comprehension models\n",
    "â”‚   â”‚      â€¢ Machine translation\n",
    "â”‚   â”‚      â€¢ Information extraction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4446a6",
   "metadata": {},
   "source": [
    "```perl\n",
    "    â”‚\n",
    "    â””â”€â”€ ğŸ”¹ Textual Inference & Meaning Representation\n",
    "        â”œâ”€ Meaning: Checks if one sentence implies/contradicts another\n",
    "        â”œâ”€ Example:\n",
    "        â”‚     Premise:   \"Riya bought a car.\"\n",
    "        â”‚     Hypothesis:\"Riya owns a car.\" â†’ Entailment = True        \n",
    "        â”œâ”€ Types:\n",
    "        â”‚     â€¢ Entailment\n",
    "        â”‚     â€¢ Contradiction\n",
    "        â”‚     â€¢ Neutral        \n",
    "        â”œâ”€ Techniques â†’ Logical forms, embeddings, transformers\n",
    "        â””â”€ Applications â†’ QA, NLI, Legal/financial reasoning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9c66b",
   "metadata": {},
   "source": [
    "```perl\n",
    " 3ï¸âƒ£ SEMANTIC PROCESSING (Meaning-Level)\n",
    "    â”‚\n",
    "    â”œâ”€â”€ ğŸ”¹ Text Representation & Unsupervised Modeling\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€â”€ 1ï¸âƒ£ Bag-of-Words (BoW)\n",
    "    â”‚   â”‚   â”œâ”€ Meaning: Represents text as word-frequency vectors. Order ignored.\n",
    "    â”‚   â”‚   â”œâ”€ Steps:\n",
    "    â”‚   â”‚   â”‚     â€¢ Build vocabulary of unique words  \n",
    "    â”‚   â”‚   â”‚     â€¢ Represent each document as counts  \n",
    "    â”‚   â”‚   â”œâ”€ Example:\n",
    "    â”‚   â”‚   â”‚     Sentences:  \n",
    "    â”‚   â”‚   â”‚       1) \"The cat sat\"  \n",
    "    â”‚   â”‚   â”‚       2) \"The dog sat\"  \n",
    "    â”‚   â”‚   â”‚     Vocabulary: {the, cat, dog, sat}  \n",
    "    â”‚   â”‚   â”‚     Vector1 â†’ [1,1,0,1]  \n",
    "    â”‚   â”‚   â”‚     Vector2 â†’ [1,0,1,1]\n",
    "    â”‚   â”‚   â”œâ”€ Pros: Simple, fast  \n",
    "    â”‚   â”‚   â””â”€ Cons: Ignores meaning & word order\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€â”€ 2ï¸âƒ£ TF-IDF (Term Frequency â€“ Inverse Document Frequency)\n",
    "    â”‚   â”‚   â”œâ”€ Meaning: Weights important words; reduces impact of common words.\n",
    "    â”‚   â”‚   â”œâ”€ Formula:\n",
    "    â”‚   â”‚   â”‚     TF(word)   = count(word in doc) / total words  \n",
    "    â”‚   â”‚   â”‚     IDF(word)  = log( N / df(word) )  i.e. log(total number of docs / num of docs with the word)\n",
    "    â”‚   â”‚   â”‚     TF-IDF     = TF Ã— IDF\n",
    "    â”‚   â”‚   â”œâ”€ Example:\n",
    "    â”‚   â”‚   â”‚     - â€œmachineâ€ appears 5 times in doc1 but rarely elsewhere â†’ high TF-IDF  \n",
    "    â”‚   â”‚   â”‚     - â€œtheâ€ appears everywhere â†’ low TF-IDF  \n",
    "    â”‚   â”‚   â”œâ”€ Pros: Highlights discriminative words  \n",
    "    â”‚   â”‚   â””â”€ Used for: Search engines, keyword extraction\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€â”€ 3ï¸âƒ£ Word Embeddings (Dense Semantic Vectors)\n",
    "    â”‚   â”‚   â”œâ”€ Meaning: Maps words â†’ continuous vectors capturing meaning.\n",
    "    â”‚   â”‚   â”œâ”€ Types:\n",
    "    â”‚   â”‚   â”‚     â€¢ Word2Vec (Skip-gram / CBOW)  \n",
    "    â”‚   â”‚   â”‚     â€¢ GloVe (Global Vectors â†’ uses co-occurrence matrix)  \n",
    "    â”‚   â”‚   â”‚     â€¢ Doc2Vec (Extend word embeddings to whole doc or sentenses as vectors - documents clustering)  \n",
    "    â”‚   â”‚   â”‚     â€¢ FastText (subword-level)  \n",
    "    â”‚   â”‚   â”‚     â€¢ Contextual Models (ELMo, BERT â†’ different vector per sentence)\n",
    "    â”‚   â”‚   â”œâ”€ Example (Word2Vec analogy):\n",
    "    â”‚   â”‚   â”‚     vector(\"king\") â€“ vector(\"man\") + vector(\"woman\") â‰ˆ vector(\"queen\")\n",
    "    â”‚   â”‚   â”‚      - (meaning: From vector king, remove masculine and add femine, gets close to vector queen)    \n",
    "    â”‚   â”‚   â”‚     vector(chef) âˆ’ vector(kitchen) + vector(park) â‰ˆ vector(coach)\n",
    "    â”‚   â”‚   â”‚      - (meaning: chef relates to kitchen as coach relates to park \n",
    "    â”‚   â”‚   â”‚          â€” analogical reasoning possible in embeddings)\n",
    "    â”‚   â”‚   â”œâ”€ Example (Doc2Vec documents clustering):\n",
    "    â”‚   â”‚   â”‚        - Doc A: â€œThe stock market rose as investors regained confidence.â€\n",
    "    â”‚   â”‚   â”‚        - Doc B: â€œThe local team won the championship last night.â€\n",
    "    â”‚   â”‚   â”‚        - Doc2Vec will place Doc A and Doc B far apart in semantic space.    \n",
    "    â”‚   â”‚   â”œâ”€ Pros: Captures semantic similarity  \n",
    "    â”‚   â”‚   â””â”€ Used for: NER, SRL, QA, ChatGPT-style models\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€â”€ 4ï¸âƒ£ Distributional Semantic Models\n",
    "    â”‚   â”‚   â”œâ”€ Based on â€œYou shall know a word by the company it keeps.â€\n",
    "    â”‚   â”‚   â”œâ”€ Methods:\n",
    "    â”‚   â”‚   â”‚     â€¢ Co-occurrence Matrix (word Ã— context count)  \n",
    "    â”‚   â”‚   â”‚     â€¢ PMI / PPMI (Pointwise Mutual Information)\n",
    "    â”‚   â”‚   â”‚         PMI(w, c) = log2( P(w,c) / (P(w)P(c)) )  \n",
    "    â”‚   â”‚   â”‚         PPMI = max(PMI, 0)\n",
    "    â”‚   â”‚   â”œâ”€ Example:\n",
    "    â”‚   â”‚   â”‚     - \"doctor\" & \"nurse\" appear in similar contexts â†’ high similarity  \n",
    "    â”‚   â”‚   â”‚     - computed via cosine similarity of vectors\n",
    "    â”‚   â”‚   â””â”€ Used for: WSD, similarity search, thesaurus generation\n",
    "    â”‚   â”‚\n",
    "    â”‚   â”œâ”€â”€ 5ï¸âƒ£ Topic Modeling (Unsupervised)\n",
    "    â”‚   â”‚   â”‚\n",
    "    â”‚   â”‚   â”œâ”€ A) LSA (Latent Semantic Analysis)\n",
    "    â”‚   â”‚   â”‚     â€¢ Uses SVD on TF-IDF matrix  \n",
    "    â”‚   â”‚   â”‚     â€¢ Reduces dimensions â†’ finds latent semantic topics  \n",
    "    â”‚   â”‚   â”‚     Example Topics:\n",
    "    â”‚   â”‚   â”‚         Topic 1: {engine, wheel, car}  \n",
    "    â”‚   â”‚   â”‚         Topic 2: {doctor, hospital, nurse}\n",
    "    â”‚   â”‚   â”‚\n",
    "    â”‚   â”‚   â”œâ”€ B) LDA (Latent Dirichlet Allocation)\n",
    "    â”‚   â”‚   â”‚     â€¢ Generative probabilistic model  \n",
    "    â”‚   â”‚   â”‚     â€¢ Each document = mixture of topics  \n",
    "    â”‚   â”‚   â”‚     â€¢ Each topic = mixture of words  \n",
    "    â”‚   â”‚   â”‚     Example:\n",
    "    â”‚   â”‚   â”‚         Doc: â€œElections are coming soonâ€  \n",
    "    â”‚   â”‚   â”‚         â†’ 70% Politics + 30% Economy\n",
    "    â”‚   â”‚   â”‚\n",
    "    â”‚   â”‚   â”œâ”€ C) NMF (Non-negative Matrix Factorization)\n",
    "    â”‚   â”‚   â”‚     â€¢ Factorizes TF-IDF into topics with non-negative weights  \n",
    "    â”‚   â”‚   â”‚\n",
    "    â”‚   â”‚   â””â”€ Used for: Summarization, clustering, search, analytics\n",
    "    â”‚   â”‚\n",
    "    â”‚   â””â”€â”€ Summary\n",
    "    â”‚       â”œâ”€ BoW â†’ counts  \n",
    "    â”‚       â”œâ”€ TF-IDF â†’ weighted counts  \n",
    "    â”‚       â”œâ”€ Embeddings â†’ dense semantic vectors  \n",
    "    â”‚       â”œâ”€ Distributional â†’ context co-occurrence  \n",
    "    â”‚       â””â”€ Topic Modeling â†’ group words into semantic topics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d560148c",
   "metadata": {},
   "source": [
    "```perl\n",
    "â”‚   â”œâ”€â”€ ğŸ”¹ NMF Topic Modeling (Non-Negative Matrix Factorization)\n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”œâ”€ Meaning:\n",
    "â”‚   â”‚   â”‚     Factorizes a **TF-IDF matrix** into:\n",
    "â”‚   â”‚   â”‚       â€¢ W  = Documentâ€“Topic matrix  \n",
    "â”‚   â”‚   â”‚       â€¢ H  = Topicâ€“Word matrix  \n",
    "â”‚   â”‚   â”‚     Such that:  TFIDF â‰ˆ W Ã— H   (all values â‰¥ 0)\n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”œâ”€ Why Non-Negative?\n",
    "â”‚   â”‚   â”‚     â€¢ Ensures topics = additive combinations of words  \n",
    "â”‚   â”‚   â”‚     â€¢ Produces interpretable parts-based representations  \n",
    "â”‚   â”‚   â”‚     â€¢ No negative weights (unlike PCA/LDA components)\n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”œâ”€ Factorization:\n",
    "â”‚   â”‚   â”‚     Let TFIDF matrix be:\n",
    "â”‚   â”‚   â”‚           M  (documents Ã— vocab)\n",
    "â”‚   â”‚   â”‚     NMF learns:\n",
    "â”‚   â”‚   â”‚           W (documents Ã— topics)\n",
    "â”‚   â”‚   â”‚           H (topics Ã— vocab)\n",
    "â”‚   â”‚   â”‚     So that:\n",
    "â”‚   â”‚   â”‚           M â‰ˆ W Ã— H\n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”œâ”€ Interpretation:\n",
    "â”‚   â”‚   â”‚     â€¢ Each row of H = a topic â†’ top words by weight  \n",
    "â”‚   â”‚   â”‚     â€¢ Each row of W = a document â†’ topic mixture weights  \n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”œâ”€ Example:\n",
    "â”‚   â”‚   â”‚     Suppose 3 topics, vocab = [food, recipe, politics, vote, economy]\n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”‚     Topicâ€“Word Matrix (H):\n",
    "â”‚   â”‚   â”‚       Topic 1 â†’ food(0.8), recipe(0.7), economy(0.1)\n",
    "â”‚   â”‚   â”‚       Topic 2 â†’ vote(0.9), politics(0.8)\n",
    "â”‚   â”‚   â”‚       Topic 3 â†’ economy(0.7), politics(0.3)\n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”‚     Interpretation:\n",
    "â”‚   â”‚   â”‚       â€¢ Topic 1 = cooking  \n",
    "â”‚   â”‚   â”‚       â€¢ Topic 2 = elections  \n",
    "â”‚   â”‚   â”‚       â€¢ Topic 3 = economics  \n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”œâ”€ Documentâ€“Topic Matrix (W):\n",
    "â”‚   â”‚   â”‚     Document A â†’ [0.9, 0.1, 0.0] â†’ mostly cooking  \n",
    "â”‚   â”‚   â”‚     Document B â†’ [0.0, 0.8, 0.2] â†’ mix of elections + economics\n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â”œâ”€ Advantages:\n",
    "â”‚   â”‚   â”‚     â€¢ Works extremely well with **TF-IDF**  \n",
    "â”‚   â”‚   â”‚     â€¢ Fast, interpretable  \n",
    "â”‚   â”‚   â”‚     â€¢ No prior distribution assumptions (unlike LDA)\n",
    "â”‚   â”‚   â”‚\n",
    "â”‚   â”‚   â””â”€ Use-Cases:\n",
    "â”‚   â”‚         â€¢ Topic discovery  \n",
    "â”‚   â”‚         â€¢ Document clustering  \n",
    "â”‚   â”‚         â€¢ Dimensionality reduction  \n",
    "â”‚   â”‚         â€¢ Extracting interpretable word groups\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9446c",
   "metadata": {},
   "source": [
    "```perl\n",
    "NLG & MACHINE TRANSLATION (Text Generation & Cross-Lingual NLP)\n",
    "â”‚\n",
    "â”œâ”€â”€ 1ï¸âƒ£ NATURAL LANGUAGE GENERATION (NLG)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Meaning: Automatically generates human-like text from structured or unstructured data.\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Pipeline (Classical NLG)\n",
    "â”‚   â”‚     â€¢ Content Determination  â†’ what to say\n",
    "â”‚   â”‚     â€¢ Text Structuring       â†’ order of ideas\n",
    "â”‚   â”‚     â€¢ Sentence Aggregation   â†’ merge concepts into sentences\n",
    "â”‚   â”‚     â€¢ Lexicalization         â†’ choose words\n",
    "â”‚   â”‚     â€¢ Surface Realization    â†’ grammar, morphology\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Approaches:\n",
    "â”‚   â”‚     â€¢ Template-Based NLG\n",
    "â”‚   â”‚          - Predefined patterns\n",
    "â”‚   â”‚          - Example: \"The temperature is <X> degrees.\"\n",
    "â”‚   â”‚     â€¢ Rule-Based NLG\n",
    "â”‚   â”‚          - Grammar + dictionary rules\n",
    "â”‚   â”‚     â€¢ Statistical N-gram Generation\n",
    "â”‚   â”‚          - Uses P(word | previous n words)\n",
    "â”‚   â”‚          - Example: bigram P(dog | the), trigram P(dog | the brown)\n",
    "â”‚   â”‚     â€¢ Neural NLG (Modern)\n",
    "â”‚   â”‚          - RNN â†’ LSTM â†’ Transformers (GPT/BERT-decoders)\n",
    "â”‚   â”‚          - Learns long-range dependencies and structure\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Key Concepts (Exam Must-Know)\n",
    "â”‚   â”‚     â€¢ N-gram Model\n",
    "â”‚   â”‚          - Definition: sequence of n words\n",
    "â”‚   â”‚          - Probability: P(w1â€¦wn) â‰ˆ Î  P(wi | wi-(n-1)â€¦ wi-1)\n",
    "â”‚   â”‚     â€¢ Laplace/Lidstone Smoothing\n",
    "â”‚   â”‚          - Prevents zero probability\n",
    "â”‚   â”‚          - Formula: P(w) = (count(w) + 1) / (N + V)\n",
    "â”‚   â”‚     â€¢ Perplexity\n",
    "â”‚   â”‚          - Measures how well model predicts unseen text\n",
    "â”‚   â”‚          - Lower = better\n",
    "â”‚   â”‚          - PP = 2^(Cross-Entropy)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Evaluation Metrics:\n",
    "â”‚   â”‚     â€¢ BLEU (precision-based n-gram overlap)\n",
    "â”‚   â”‚     â€¢ ROUGE (recall-based overlap)\n",
    "â”‚   â”‚     â€¢ METEOR (synonym-aware evaluation)\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€ Applications:\n",
    "â”‚         â€¢ Chatbots\n",
    "â”‚         â€¢ Report generation\n",
    "â”‚         â€¢ Summarization (abstractive)\n",
    "â”‚         â€¢ Story generation\n",
    "â”‚         â€¢ Dialogue systems\n",
    "â”‚\n",
    "â”œâ”€â”€ 2ï¸âƒ£ MACHINE TRANSLATION (MT)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Meaning: Converting text from one language (L1) to another (L2)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ A. Rule-Based Machine Translation (RBMT)\n",
    "â”‚   â”‚     â€¢ Oldest MT\n",
    "â”‚   â”‚     â€¢ Uses bilingual dictionaries + grammar rules\n",
    "â”‚   â”‚     â€¢ Types:\n",
    "â”‚   â”‚         - Direct Translation (word-for-word)\n",
    "â”‚   â”‚         - Transfer-Based (syntax trees â†’ intermediate structure)\n",
    "â”‚   â”‚         - Interlingua-Based (universal meaning form)\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ B. Statistical Machine Translation (SMT)\n",
    "â”‚   â”‚     â€¢ Core Idea: Translation = argmax P(target | source)\n",
    "â”‚   â”‚     â€¢ Models Used:\n",
    "â”‚   â”‚         â€¢ Phrase-Based SMT (PBSMT) â† Most common\n",
    "â”‚   â”‚         â€¢ Word-Based (IBM Models)\n",
    "â”‚   â”‚         â€¢ Hierarchical SMT\n",
    "â”‚   â”‚\n",
    "â”‚   â”‚     â€¢ Components of SMT:\n",
    "â”‚   â”‚         - Translation Model  â†’ P(foreign|english)\n",
    "â”‚   â”‚         - Language Model     â†’ P(target sentence)\n",
    "â”‚   â”‚         - Decoder            â†’ chooses best translation\n",
    "â”‚   â”‚\n",
    "â”‚   â”‚     â€¢ Example:\n",
    "â”‚   â”‚         Input:  \"He is eating.\"\n",
    "â”‚   â”‚         Output: \"Er isst.\"\n",
    "â”‚   â”‚\n",
    "â”‚   â”‚     â€¢ Limitations:\n",
    "â”‚   â”‚         - Phrase table storage\n",
    "â”‚   â”‚         - Poor long-distance handling\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ C. Neural Machine Translation (NMT)\n",
    "â”‚   â”‚     â€¢ Uses an encoder-decoder neural architecture\n",
    "â”‚   â”‚     â€¢ Earlier â†’ RNN/LSTM + Attention\n",
    "â”‚   â”‚     â€¢ Now â†’ Transformers (e.g., BERT, GPT, T5)\n",
    "â”‚   â”‚\n",
    "â”‚   â”‚     â€¢ Key Concept: Attention\n",
    "â”‚   â”‚         - Aligns each target word to relevant source part\n",
    "â”‚   â”‚\n",
    "â”‚   â”‚     â€¢ Example:\n",
    "â”‚   â”‚         â€œThe cat is on the matâ€ â†’ â€œLe chat est sur le tapisâ€\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€ Evaluation Metrics (common across SMT and NMT):\n",
    "â”‚   â”‚     â€¢ BLEU  \n",
    "â”‚   â”‚     â€¢ TER (Translation Edit Rate)\n",
    "â”‚   â”‚     â€¢ METEOR\n",
    "â”‚   â”‚     â€¢ BERTScore (semantic evaluation)\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€ Applications:\n",
    "â”‚         â€¢ Google Translate\n",
    "â”‚         â€¢ Document translation\n",
    "â”‚         â€¢ Cross-lingual QA\n",
    "â”‚         â€¢ Subtitles, localization\n",
    "â”‚\n",
    "â””â”€â”€ 3ï¸âƒ£ OTHER IMPORTANT TOPICS (Exam-Relevant)\n",
    "    â”‚\n",
    "    â”œâ”€ N-gram Language Models (Used in NLG + SMT)\n",
    "    â”‚     â€¢ Bigram/trigram approximation\n",
    "    â”‚     â€¢ Markov assumption\n",
    "    â”‚     â€¢ Smoothing required\n",
    "    â”‚     â€¢ Evaluated by perplexity\n",
    "    â”‚\n",
    "    â”œâ”€ Morphological Generation\n",
    "    â”‚     â€¢ Producing correct word forms\n",
    "    â”‚     â€¢ Example: \"run\" â†’ \"running\", \"ran\"\n",
    "    â”‚\n",
    "    â”œâ”€ Alignment Models (SMT)\n",
    "    â”‚     â€¢ IBM Models 1â€“5\n",
    "    â”‚     â€¢ EM algorithm for training\n",
    "    â”‚\n",
    "    â”œâ”€ Template-Based MT / NLG\n",
    "    â”‚     â€¢ Handwritten rules + placeholders\n",
    "    â”‚     â€¢ Example:\n",
    "    â”‚         \"My name is <NAME> and I am <AGE> years old.\"\n",
    "    â”‚\n",
    "    â””â”€ Hybrid MT\n",
    "          â€¢ Combines rules + SMT/NMT\n",
    "          â€¢ Example: Domain-specific dictionaries + neural model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb1555",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
