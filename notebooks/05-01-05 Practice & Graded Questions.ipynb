{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f28ffc",
   "metadata": {},
   "source": [
    "# Practice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f636e",
   "metadata": {},
   "source": [
    "**Q1. Which of the following best describes the primary goal of lexical and syntactic processing in NLP?**\n",
    "- Converting audio to text efficiently\n",
    "- Understanding the meaning and structure of text at the word and sentence levels\n",
    "- Translating text from one language to another\n",
    "- Storing textual data for retrieval\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Understanding the meaning and structure of text at the word and sentence levels\n",
    "\n",
    "Explanation:\n",
    "- Lexical processing deals with words and their meanings ‚Äî identifying parts of speech, word forms, and vocabulary (e.g., tokenization, stemming, lemmatization).\n",
    "- Syntactic processing focuses on sentence structure ‚Äî how words combine to form grammatically valid sentences (e.g., parsing, POS tagging).\n",
    "- Together, they help NLP systems understand both the meaning and grammatical relationships within text ‚Äî forming the foundation for deeper semantic and contextual analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30834df6",
   "metadata": {},
   "source": [
    "**Q2. Which of the following linguistic levels focuses on the structure and grammatical rules governing how words combine to form sentences?**\n",
    "- Morphology\n",
    "- Syntax\n",
    "- Semantics\n",
    "- Pragmatics\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Syntax\n",
    "\n",
    "Explanation:\n",
    "- Syntax is the linguistic level that focuses on the arrangement of words and the grammatical structure of sentences.\n",
    "It determines how words combine according to language rules to form meaningful and grammatically correct sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e2040",
   "metadata": {},
   "source": [
    "**Q3. Traditional feature extraction methods such as Bag-of-Words and TF-IDF are limited because they:**\n",
    "- Are too computationally expensive\n",
    "- Fail to capture the semantic meaning and context of words\n",
    "- Do not require preprocessing like tokenisation\n",
    "- Work only for speech data\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Fail to capture the semantic meaning and context of words\n",
    "\n",
    "Explanation:\n",
    "- Traditional text representation methods like Bag-of-Words (BoW) and TF-IDF (Term Frequency‚ÄìInverse Document Frequency) represent text numerically based on word frequency, but they have major limitations:\n",
    "- They ignore word order (syntax).\n",
    "- They treat each word independently, missing how words relate to each other in context.\n",
    "- Hence, they fail to capture semantics ‚Äî for example, ‚Äúgood‚Äù and ‚Äúgreat‚Äù are treated as completely different words, even though they have similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16a1a37",
   "metadata": {},
   "source": [
    "**Q4. How do rule-based NLP systems differ from statistical or data-driven approaches?**\n",
    "- They are always more accurate. \n",
    "- They learn from large labelled data sets. \n",
    "- They depend on manually crafted linguistic rules and patterns. \n",
    "- They use neural embeddings to represent text.\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "They depend on manually crafted linguistic rules and patterns.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- Rule-based NLP systems are built on explicit linguistic rules, handcrafted by experts ‚Äî such as grammar rules, dictionaries, and pattern-matching logic.\n",
    "They rely on human knowledge, not data, to process language.\n",
    "\n",
    "- üëâ Example: A rule-based system might define: If a sentence contains ‚Äúnot‚Äù + adjective ‚Üí sentiment = negative.\n",
    "\n",
    "- These systems are deterministic ‚Äî they perform well in narrow domains but struggle with ambiguity and unseen patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05290c",
   "metadata": {},
   "source": [
    "**Q5. Which of the following is an example of a real-world NLP application relying on both lexical and syntactic processing?**\n",
    "- CPU scheduling\n",
    "- Image segmentation \n",
    "- Sentiment analysis and conversational agents \n",
    "- Database indexing\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Sentiment analysis and conversational agents\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- Both lexical and syntactic processing are crucial in understanding human language ‚Äî and sentiment analysis or chatbots (conversational agents) rely heavily on them.\n",
    "\n",
    "- Lexical processing helps identify words, their meanings, and parts of speech.\n",
    "    - ‚Üí Example: recognizing that ‚Äúhappy‚Äù and ‚Äújoyful‚Äù express positive sentiment.\n",
    "\n",
    "- Syntactic processing analyzes sentence structure to interpret relationships between words.\n",
    "    - ‚Üí Example: differentiating between ‚ÄúI didn‚Äôt like the movie‚Äù and ‚ÄúI liked the movie‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab963b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cebf3",
   "metadata": {},
   "source": [
    "**Q1. What is the primary goal of text normalisation (e.g., lowercasing, punctuation handling, and whitespace cleanup) in lexical preprocessing?**\n",
    "- To create a consistent text representation that reduces superficial variation before analysis\n",
    "- To guarantee that capitalisation-based meanings (e.g., 'US' vs 'us') are always preserved\n",
    "- To permanently remove all numbers, emojis, and punctuation from any data set \n",
    "- To replace domain knowledge so models do not need task-specific rules\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "To create a consistent text representation that reduces superficial variation before analysis\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The primary goal of text normalization is to make text uniform and comparable by reducing superficial differences that don‚Äôt affect meaning ‚Äî such as capitalization, punctuation, or extra whitespace.\n",
    "\n",
    "This helps NLP models and algorithms treat similar words (like ‚ÄúDog‚Äù and ‚Äúdog‚Äù) as the same token, improving accuracy and reducing vocabulary size.\n",
    "\n",
    "Incorrect options explained:\n",
    "\n",
    "‚ùå To guarantee that capitalisation-based meanings are preserved ‚Üí\n",
    "Normalization typically removes capitalization distinctions unless explicitly required (e.g., ‚ÄúUS‚Äù vs ‚Äúus‚Äù is a special case handled separately).\n",
    "\n",
    "‚ùå To permanently remove all numbers, emojis, and punctuation ‚Üí\n",
    "Not always ‚Äî punctuation or numbers may carry meaning (e.g., ‚ÄúC++‚Äù, ‚Äúversion 2.0‚Äù).\n",
    "\n",
    "‚ùå To replace domain knowledge ‚Üí\n",
    "Normalization doesn‚Äôt replace domain expertise ‚Äî it‚Äôs a preprocessing step, not a reasoning or modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5cd959",
   "metadata": {},
   "source": [
    "**Q2.Which scenario best motivates the use of rules/regex-aware tokenisation rather than simple whitespace splitting?**\n",
    "- Processing a corpus where every word is separated by single spaces \n",
    "- Splitting only on newline characters to obtain sentences \n",
    "- Handling abbreviations such as 'Dr', multiword expressions such as 'New York' and contractions such as 'don‚Äôt' \n",
    "- Tokenising code comments that contain only lowercase words \n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Handling abbreviations such as 'Dr', multiword expressions such as 'New York', and contractions such as 'don‚Äôt'\n",
    "\n",
    "Explanation:\n",
    "\n",
    "A rule-based or regex-aware tokenizer goes beyond simple whitespace splitting.\n",
    "It can handle complex linguistic patterns such as:\n",
    "\n",
    "Abbreviations: ‚ÄúDr.‚Äù or ‚ÄúU.S.A.‚Äù should not split after every period.\n",
    "\n",
    "Multiword expressions: ‚ÄúNew York‚Äù or ‚ÄúSan Francisco‚Äù should remain a single token in some tasks.\n",
    "\n",
    "Contractions: ‚Äúdon‚Äôt‚Äù ‚Üí [do, not] instead of [don‚Äôt].\n",
    "\n",
    "These cases require custom rules or regular expressions that detect and preserve or split tokens intelligently.\n",
    "\n",
    "Incorrect options:\n",
    "\n",
    "‚ùå Processing a corpus where every word is separated by single spaces ‚Üí Whitespace tokenization is sufficient here.\n",
    "\n",
    "‚ùå Splitting only on newline characters to obtain sentences ‚Üí That‚Äôs sentence segmentation, not tokenization.\n",
    "\n",
    "‚ùå Tokenising code comments that contain only lowercase words ‚Üí Simple whitespace splitting is enough; no regex needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88d1ef",
   "metadata": {},
   "source": [
    "**Q3. Which of the following is a sound strategy for managing stopwords across different domains?**\n",
    "- Always create a fixed, generic stopword list for every task. \n",
    "- Start with a base list and then customise for each task/domain; keep negations if the meaning depends on them. \n",
    "- Retain all high-frequency words because frequency always signals importance. \n",
    "- Remove interrogatives (such as ‚Äòwhat' and 'why') in question answering to reduce noise.\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Start with a base list and then customise for each task/domain; keep negations if the meaning depends on them.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- Stopword management should always be context-aware and domain-sensitive.\n",
    "- A good strategy is to:\n",
    "    - Begin with a generic stopword list (like NLTK or spaCy‚Äôs).\n",
    "    - Customise it for your domain or task.\n",
    "    - Retain words like ‚Äúnot‚Äù, ‚Äúnever‚Äù, or ‚Äúno‚Äù when sentiment or polarity matters.\n",
    "- This ensures that you remove true noise while preserving meaningful words relevant to your application.\n",
    "\n",
    "Incorrect options:\n",
    "\n",
    "‚ùå Always create a fixed, generic stopword list for every task ‚Üí Generic lists often remove important domain-specific words (e.g., ‚Äúpatient‚Äù in healthcare).\n",
    "\n",
    "‚ùå Retain all high-frequency words because frequency always signals importance ‚Üí Frequency ‚â† importance; common words may add noise.\n",
    "\n",
    "‚ùå Remove interrogatives in question answering ‚Üí Words like ‚Äúwhat‚Äù and ‚Äúwhy‚Äù are crucial for understanding question intent, not noise.-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f6b9b",
   "metadata": {},
   "source": [
    "**Q4. Which of the following options correctly pairs the morphological process with its example?**\n",
    "- Derivation: 'run' ‚Üí 'running' (same part-of-speech) \n",
    "- Inflection: 'teach' ‚Üí 'teacher' (verb to noun) \n",
    "- Inflection: 'run' ‚Üí 'runs' (grammatical change, core meaning preserved) \n",
    "- Derivation: 'cats' ‚Üí 'cat' (plural to singular) \n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Inflection: 'run' ‚Üí 'runs' (grammatical change, core meaning preserved)\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- Inflection changes a word‚Äôs grammatical form (like tense, number, or agreement) without altering its core meaning or part of speech.\n",
    "    üëâ Example: run ‚Üí runs, walk ‚Üí walked, cat ‚Üí cats\n",
    "\n",
    "- Derivation, on the other hand, creates a new word with a different part of speech or meaning.\n",
    "    üëâ Example: teach ‚Üí teacher (verb ‚Üí noun), happy ‚Üí happiness (adjective ‚Üí noun)\n",
    "\n",
    "Incorrect options:\n",
    "\n",
    "‚ùå Derivation: 'run' ‚Üí 'running' ‚Üí This is inflection (verb tense change).\n",
    "\n",
    "‚ùå Inflection: 'teach' ‚Üí 'teacher' ‚Üí That‚Äôs derivation (verb ‚Üí noun).\n",
    "\n",
    "‚ùå Derivation: 'cats' ‚Üí 'cat' ‚Üí That‚Äôs inflection (number change)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95784d02",
   "metadata": {},
   "source": [
    "**Q5. Which approach best preserves meaning while reducing word forms and handling irregular variants?** \n",
    "- Lemmatisation that uses a lexicon and words role in a sentence\n",
    "- Porter/Snowball stemming that truncates suffixes via rule sets\n",
    "- Edit distance that counts character edits between two strings\n",
    "- Noisy channel scoring that ranks candidate corrections by prior and error likelihood\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Lemmatisation that uses a lexicon and a word‚Äôs role in a sentence\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Lemmatization reduces words to their dictionary base form (lemma) using morphological analysis and part-of-speech (POS) information.\n",
    "- üëâ Example:\n",
    "    - studies ‚Üí study\n",
    "    - better ‚Üí good\n",
    "    - was ‚Üí be\n",
    "\n",
    "It preserves meaning because it understands the grammatical role and context, handling even irregular forms accurately (e.g., went ‚Üí go).\n",
    "\n",
    "Incorrect options:\n",
    "\n",
    "‚ùå Porter/Snowball stemming ‚Üí Strips suffixes by fixed rules; fast but may yield non-words (e.g., studies ‚Üí studi).\n",
    "\n",
    "‚ùå Edit distance ‚Üí Measures string similarity, not meaning preservation.\n",
    "\n",
    "‚ùå Noisy channel scoring ‚Üí Used for spell correction, not for reducing morphological variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7d8d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63d55d",
   "metadata": {},
   "source": [
    "**Q1. What is the primary objective of syntactic analysis in NLP?**\n",
    "- To identify how words combine into structures that express grammatical relations \n",
    "- To translate sentences between languages word by word \n",
    "- To convert audio signals into text transcripts \n",
    "- To remove uninformative words from text\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "To identify how words combine into structures that express grammatical relations\n",
    "\n",
    "Explanation:\n",
    "- Syntactic analysis (or parsing) is the stage in NLP that determines how words are related and organized in a sentence according to grammar rules. It focuses on:\n",
    "    - Sentence structure (who did what to whom)\n",
    "    - Relationships between words (subject, verb, object)\n",
    "    - Building parse trees or dependency graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e0c9ee",
   "metadata": {},
   "source": [
    "**Q2. Why is Part-of-Speech (POS) tagging important for downstream NLP tasks?**\n",
    "- It assigns grammatical categories to tokens, enabling parsing and information extraction.\n",
    "- It guarantees accurate sentence meaning without context. \n",
    "- It replaces the need for any grammar or parsing algorithms.\n",
    "- It is only useful for languages without word boundaries. \n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "It assigns grammatical categories to tokens, enabling parsing and information extraction.\n",
    "\n",
    "Explanation:\n",
    "- Part-of-Speech (POS) tagging labels each word in a sentence with its grammatical role ‚Äî such as noun, verb, adjective, adverb, etc.\n",
    "- This grammatical information forms the foundation for higher-level NLP tasks like parsing, named entity recognition (NER), machine translation, and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd219f",
   "metadata": {},
   "source": [
    "**Q3. Which of the following statements best distinguishes common POS tagging approaches?**\n",
    "- Rule-based uses handcrafted rules, HMM learns tag sequences probabilistically and Brill learns error-fixing transformations.\n",
    "- Rule-based predicts tags with neural embeddings, HMM relies on deterministic suffix rules and Brill is unsupervised clustering. \n",
    "- Rule-based requires no lexicon, HMM ignores tag sequences and Brill applies random corrections.\n",
    "- All three methods work identically but differ only in speed. \n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Rule-based uses handcrafted rules, HMM learns tag sequences probabilistically, and Brill learns error-fixing transformations.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "There are three major approaches to Part-of-Speech (POS) tagging, and they differ mainly in how they determine the correct tag for each word:\n",
    "\n",
    "| **Approach**                                    | **Core Idea**                                                                                             | **Example / Mechanism**                                                | **Key Feature**                     |\n",
    "| ----------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | ----------------------------------- |\n",
    "| **Rule-Based Tagging**                          | Uses manually written linguistic rules + lexicons.                                                        | ‚ÄúIf word ends with *-ly*, tag as adverb (RB).‚Äù                         | Deterministic, interpretable.       |\n",
    "| **Statistical Tagging (HMM)**                   | Learns tag probabilities from data ‚Äî based on **P(tag given previous tag)** and **P(word given tag)**.    | Uses the **Viterbi algorithm** to find the most probable tag sequence. | Data-driven, probabilistic.         |\n",
    "| **Transformation-Based Tagging (Brill Tagger)** | Starts with a simple baseline (e.g., most frequent tag) and **learns rules that fix errors** iteratively. | ‚ÄúIf current tag is NN but previous tag is TO ‚Üí change to VB.‚Äù          | Hybrid ‚Äî combines rules + learning. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f002651",
   "metadata": {},
   "source": [
    "**Q4. What is a key difference between constituency and dependency parsing?**\n",
    "- Constituency groups words into phrase nodes, and dependency links words directly via head-dependent relations. \n",
    "- Constituency uses only POS tags, and dependency uses only characters. \n",
    "- Constituency cannot represent ambiguity, and dependency always produces multiple trees.\n",
    "- Constituency is for speech only, and dependency is for written text only. \n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Constituency groups words into phrase nodes, and dependency links words directly via head-dependent relations.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Both Constituency Parsing and Dependency Parsing analyze sentence structure,\n",
    "but they represent that structure in different ways üëá\n",
    "\n",
    "| **Aspect**              | **Constituency Parsing**                                       | **Dependency Parsing**                                   |\n",
    "| ----------------------- | -------------------------------------------------------------- | -------------------------------------------------------- |\n",
    "| **Focus**               | Groups words into **phrases** (constituents) like NP, VP, PP.  | Connects words via **head‚Äìdependent** grammatical links. |\n",
    "| **Representation**      | Phrase structure tree (hierarchical).                          | Directed graph (relations between words).                |\n",
    "| **Based On**            | **Context-Free Grammar (CFG)** rules.                          | **Dependency Grammar**.                                  |\n",
    "| **Nodes Represent**     | Phrases (e.g., NP = ‚ÄúThe quick fox‚Äù).                          | Words (each node = a word).                              |\n",
    "| **Relations Represent** | Containment (part-of-phrase).                                  | Grammatical function (subject, object, modifier).        |\n",
    "| **Example Sentence**    | ‚ÄúThe quick brown fox jumps over the dog.‚Äù                      | ‚Äî                                                        |\n",
    "| **Constituency Output** | [S [NP The quick brown fox] [VP jumps [PP over [NP the dog]]]] | ‚Äî                                                        |\n",
    "| **Dependency Output**   | jumps ‚Üí fox (nsubj); jumps ‚Üí dog (pobj); jumps ‚Üí over (prep)   | ‚Äî                                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e7bb9",
   "metadata": {},
   "source": [
    "**Q5. Which of the following examples illustrates grammatical agreement checking in syntactic analysis?**\n",
    "- Detecting that 'These houses is old' should be 'These houses are old'\n",
    "- Lowercasing 'The' to 'the' for consistency\n",
    "- Removing the stopwords 'the' and 'is' to reduce noise\n",
    "- Measuring edit distance between 'form' and 'from' \n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Detecting that 'These houses is old' should be 'These houses are old'\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Grammatical agreement checking ensures that words in a sentence match correctly in number, person, gender, or tense ‚Äî so the sentence follows grammar rules.\n",
    "| **Type of Agreement** | **Correct Example (‚úî)**   | **Incorrect Example (‚úò)** | **Explanation**                         |\n",
    "| --------------------- | ------------------------- | ------------------------- | --------------------------------------- |\n",
    "| Subject‚ÄìVerb          | These houses **are** old. | These houses **is** old.  | Plural subject ‚Üí plural verb.           |\n",
    "| Determiner‚ÄìNoun       | **This apple** is red.    | **These apple** is red.   | Determiner must match noun in number.   |\n",
    "| Pronoun‚ÄìNoun          | John lost **his** pen.    | John lost **her** pen.    | Pronoun must match antecedent‚Äôs gender. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533eda4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7020a958",
   "metadata": {},
   "source": [
    "# Graded Qeuestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dda568",
   "metadata": {},
   "source": [
    "**Q1. Which of the following steps most directly converts cleaned text into a machine-usable numeric form for modelling in a classic NLP pipeline?**\n",
    "- Tokenisation\n",
    "- Feature extraction (e.g., Bag-of-Words, TF-IDF and embeddings) \n",
    "- Modelling\n",
    "- Stopword removal\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Feature extraction (e.g., Bag-of-Words, TF-IDF, and embeddings)\n",
    "\n",
    "Explanation:\n",
    "- Tokenisation splits text into words or subwords (still textual).\n",
    "- Stopword removal filters out common but uninformative words.\n",
    "- Modelling comes after numerical representation is ready.\n",
    "\n",
    "‚úÖ Feature extraction is the key step that converts text into numbers ‚Äî vectors that machine learning models can understand.\n",
    "\n",
    "| Technique           | Description                                          | Example Output                 |\n",
    "| ------------------- | ---------------------------------------------------- | ------------------------------ |\n",
    "| **Bag-of-Words**    | Counts how many times each word appears.             | `[2, 0, 1, 0, 3]`              |\n",
    "| **TF‚ÄìIDF**          | Weights words by importance in the corpus.           | `[0.21, 0.0, 0.56, 0.0, 0.12]` |\n",
    "| **Word Embeddings** | Represents words as dense vectors capturing meaning. | `[0.32, -0.47, 0.88, ‚Ä¶]`       |\n",
    "\n",
    "‚û°Ô∏è In short:\n",
    "\n",
    "Feature extraction = the bridge between raw text and numerical models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4414fe",
   "metadata": {},
   "source": [
    "**Q2. Which of the following scenarios most strongly suggests using regex/rule-based tokenisation (or a robust library tokeniser) instead of simple whitespace splitting?**\n",
    "- Clean English text with single spaces and no abbreviations \n",
    "- Text with 'Dr', '3.14', 'don‚Äôt' and multiword expressions such as 'New York' \n",
    "- A list of single, lowercase keywords separated by spaces \n",
    "- Short labels that are already pre-separated and punctuation-free\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Text with 'Dr', '3.14', 'don‚Äôt' and multiword expressions such as 'New York'\n",
    "\n",
    "Explanation:\n",
    "| **Scenario**                                      | **Why or Why Not**                                                                                                                                                                                                                 |\n",
    "| ------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Clean English text with single spaces**         | Simple whitespace splitting is fine ‚Äî no tricky boundaries.                                                                                                                                                                        |\n",
    "| **‚úÖ Text with 'Dr', '3.14', 'don‚Äôt', 'New York'** | Needs regex/rule-based tokenisation to handle: <br>‚Ä¢ Abbreviations (`Dr.` not end of sentence)<br>‚Ä¢ Decimals (`3.14` shouldn‚Äôt split)<br>‚Ä¢ Contractions (`don‚Äôt` ‚Üí ‚Äúdo‚Äù + ‚Äúnot‚Äù)<br>‚Ä¢ Multiword entities (`New York` ‚Üí one token). |\n",
    "| **Single lowercase keywords**                     | Already clean ‚Äî whitespace splitting is enough.                                                                                                                                                                                    |\n",
    "| **Short labels, punctuation-free**                | No special structure ‚Äî simple split works fine.                                                                                                                                                                                    |\n",
    "In short:\n",
    "\n",
    "Use regex/rule-based or library tokenisers (like spaCy, NLTK, or TweetTokenizer)\n",
    "when text contains abbreviations, punctuation, contractions, or multiword phrases that whitespace splitting would mishandle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45825d13",
   "metadata": {},
   "source": [
    "**Q3. Suppose you are building a high-precision QA system. Which preprocessing choice is most appropriate for preserving meaning while unifying word forms?**\n",
    "- Use Porter/Snowball stemming for speed, accepting non-word stems\n",
    "- Use dictionary-based lemmatisation with POS, accepting a higher compute cost \n",
    "- Remove all suffixes manually with a fixed list, ignoring context \n",
    "- Skip any reduction; rely only on raw surface forms \n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Use dictionary-based lemmatisation with POS, accepting a higher compute cost\n",
    "\n",
    "| **Option**                   | **Description**                                                                                                   | **Impact on QA System**                                                            |\n",
    "| ---------------------------- | ----------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n",
    "| **Porter/Snowball Stemming** | Fast, rule-based suffix stripping; often produces non-words (e.g., *studies ‚Üí studi*)                             | ‚ùå Loses linguistic accuracy ‚Üí harms meaning understanding.                         |\n",
    "| **‚úÖ Lemmatisation with POS** | Reduces words to valid dictionary forms (e.g., *studies ‚Üí study*, *better ‚Üí good*) using morphological + POS info | ‚úÖ Preserves semantics, ideal for QA, translation, or semantic reasoning.           |\n",
    "| **Manual suffix removal**    | Crude and context-insensitive                                                                                     | ‚ùå Breaks accuracy; cannot handle irregulars (*went ‚Üí go* missed).                  |\n",
    "| **Skip reduction**           | Keeps original forms                                                                                              | ‚ö†Ô∏è Works for small tasks but fails to match *run/running/ran* as the same concept. |\n",
    "\n",
    "In short:\n",
    "\n",
    "For meaning-critical NLP tasks (like Question Answering, Machine Translation, or Inference),\n",
    "lemmatisation is preferred because it preserves linguistic correctness and semantic meaning ‚Äî even if it‚Äôs slower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2988f02d",
   "metadata": {},
   "source": [
    "**Q4. Which of the following statements best contrasts rule-based, HMM and Brill POS taggers?**\n",
    "- Rule-based: Hand-written rules; HMM: Probabilistic sequence model; Brill: Learns ordered error-correcting tansformations\n",
    "- Rule-based: Neural embeddings; HMM: Character rules only; Brill: Random rewrites\n",
    "- Rule-based: No lexicon; HMM: Ignores tag transitions; Brill: Unsupervised clustering \n",
    "- All three are identical, and the differences are only in the runtime speed.\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "Rule-based: Hand-written rules; HMM: Probabilistic sequence model; Brill: Learns ordered error-correcting transformations\n",
    "\n",
    "Explanation:\n",
    "| **Approach**                            | **How It Works**                                                                  | **Key Idea**                                  | **Example / Analogy**                                   |                                |                                                                                 |\n",
    "| --------------------------------------- | --------------------------------------------------------------------------------- | --------------------------------------------- | ------------------------------------------------------- | ------------------------------ | ------------------------------------------------------------------------------- |\n",
    "| **Rule-Based Tagger**                   | Uses manually written grammar and lexical rules.                                  | Deterministic linguistic logic.               | ‚ÄúIf word ends with *-ly*, tag as Adverb (RB).‚Äù          |                                |                                                                                 |\n",
    "| **HMM Tagger**                          | Learns probabilities of tag sequences *(P(tag                                     | prev_tag))* and word-tag pairs *(P(word       | tag))*.                                                 | Data-driven statistical model. | ‚ÄúNoun is likely after a Determiner; ‚Äòrun‚Äô ‚Üí Verb or Noun depending on context.‚Äù |\n",
    "| **Brill Tagger (Transformation-Based)** | Starts with a simple baseline tagger, then **learns correction rules** from data. | Combines rule clarity + statistical learning. | ‚ÄúIf current tag=NN and previous tag=TO ‚Üí change to VB.‚Äù |                                |                                                                                 |\n",
    "\n",
    "In short:\n",
    "\n",
    "üß© Rule-based = handcrafted grammar\n",
    "üé≤ HMM = probabilistic sequences\n",
    "üîÅ Brill = learned correction rules (hybrid of both worlds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b8a87",
   "metadata": {},
   "source": [
    "**Q5. Which pairing correctly matches the method with its primary representation or goal in syntactic analysis?**\n",
    "- Constituency parsing ‚Üí Head-dependent arcs without phrase nodes\n",
    "- Dependency parsing ‚Üí Phrase-structure trees using CFG rule expansions \n",
    "- PCFGs ‚Üí Choose among multiple valid trees by assigning probabilities to grammar rules \n",
    "- Chunking ‚Üí Deep, fully nested trees that resolve long-distance dependencies\n",
    "\n",
    "‚úÖ Correct Answer:\n",
    "PCFGs ‚Üí Choose among multiple valid trees by assigning probabilities to grammar rules\n",
    "\n",
    "| **Option**                                                                                    | **Concept**     | **True/False**                                                                                                                                                                                                                                  | **Explanation** |\n",
    "| --------------------------------------------------------------------------------------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- |\n",
    "| **1Ô∏è‚É£ Constituency parsing ‚Üí Head-dependent arcs without phrase nodes**                       | ‚ùå **Incorrect** | Constituency parsing uses **phrase nodes** (like NP, VP, PP). It produces **phrase structure trees**, *not* head-dependent arcs ‚Äî that‚Äôs **dependency parsing**.                                                                                |                 |\n",
    "| **2Ô∏è‚É£ Dependency parsing ‚Üí Phrase-structure trees using CFG rule expansions**                 | ‚ùå **Incorrect** | Dependency parsing builds **relations between words** (like *dog ‚Üí chased (nsubj)*). It doesn‚Äôt use **CFG rules** or **phrase nodes**. CFG expansions are used in **constituency parsing**.                                                     |                 |\n",
    "| **3Ô∏è‚É£ PCFGs ‚Üí Choose among multiple valid trees by assigning probabilities to grammar rules** | ‚úÖ **Correct**   | PCFG = **Probabilistic Context-Free Grammar**. It attaches **probabilities to CFG production rules** (like `NP ‚Üí DT NN [0.4]`, `VP ‚Üí VB NP [0.6]`). When a sentence has **multiple valid parse trees**, PCFG selects the **most probable one**. |                 |\n",
    "| **4Ô∏è‚É£ Chunking ‚Üí Deep, fully nested trees that resolve long-distance dependencies**           | ‚ùå **Incorrect** | Chunking = **shallow parsing**, producing flat phrase chunks (e.g., `[NP The old man] [VP sat] [PP on the bench]`). It **does not** build deep or recursive trees or capture long-range dependencies.                                           |                 |\n",
    "\n",
    "\n",
    "In short:\n",
    "\n",
    "üß© PCFGs = CFG + probabilities ‚Üí choose the most plausible syntactic structure among possible parse trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc8981",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
